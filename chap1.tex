\chapter{Biological V1 neurons and theories for how they compute}

\section{The philosophy and the approach}
Studying the vision might not be the most effective modality for understanding how the brain processes natural signals. As far as sensor modalities are concerned, olfaction has a smaller input dimensionality and more limited range of possible input parameters. Auditory signals are generally easier to probe and represent, and the auditory transduction system is better understood than the retina. Like vision, audition requires solving ill-posed problems of inference and selective attention; understanding how this is accomplished would provide great insight to neural computation. Vision, however, is confounded by an extremely complicated neural input array, complex optical transformations, incessant motion of the eye, and a three dimensional input space, among other nuances. So why is vision the most studied perception? One answer could be that those who study vision do so because it is their most prized and intuitive sense. With vision we can determine predator from prey, establish social bonds, and navigate our treacherous world. The visual sense is powerful, in that it typically dominates cross-modality inputs for assessing physical properties of objects [Ernst and Banks, 2002]. Unfortunately, the immediate and compelling nature of our visual percept can also bias our investigations [Rodieck, 1998]. Vision is intuitive for humans, which makes it difficult to grasp the complicated nature of deriving our perception from light reflections in the world. Indeed, early machine vision work approached the problem from what is in hindsight a simplistic perspective [Minsky, 1966]. Much of the history of studying vision has been dominated by the notion that it is simply a passive sense facilitated by filters and feature detectors. The powerful impact that vision has on our mental state makes introspection difficult to reconcile with objective scientific analysis. Vision research is fraught inherent complications, unsolved problems, and unintuitive answer, but it is also the source of compelling examples of fascinating phenomena in human perception. Vision research is motivated by our ability to perceive over 10 orders of magnitude of illuminance levels [Norton, pg 76], untangle complicated inverse graphics problems such as shape from shading and edge detection, rapidly recognize objects and yet also demonstrate extreme robustness to input noise and distributional shift. We are also inspired by instances where we fail to perceive the stimulus as it is physically, in the form of illusions, bi-stable percepts, and metamers. These phenomena are accessible and compelling to anyone, and drive the field at large and the work herein.

It is well understood that there exists a dichotomy between how we perceive our world and the actual information impinging our sensory systems. For example, our eyes are constantly in motion, even when we fixate our gaze upon a stationary object, and yet the fixated object appears stable. The retina itself gives us another example with uneven sampling of visual information resulting in a uniformly resolved perception. Questions then arise about what sort of signal processing is the brain performing to allow us to perceive the world as we do and determine the ecologically salient features of the world. 

Neuroscience is a discipline that spans many levels of understanding in order to investigate how the nervous system works. The field is broad, so before continuing it is important to narrow the scope. In David Marr's 1982 book, \textit{Vision} \cite{marr1982vision}, he defines three levels of understanding that since have been used heavily to motivate theoretical neuroscience research. These levels are Computational Theory, Representation and Algorithm, and Hardware Implementation. Importantly, Marr asserts that all three levels must be understood before one can emulate the entity under study. In our case, we wish to understand how brains work and how to build a brain. The answers to these questions are vast, but here we provide a contribution. We will investigate the computations underlying visual perception by analyzing models of neural coding and signal representation.

This thesis investigates how lateral connectivity and recurrent inference affects image coding properties of model neurons. It presents an argument for increasing the complexity of model neurons in neuroscience and machine learning research. It supports this argument by improving upon recent methods for increasing the degrees of nonlinearity in neuron models without significantly decreasing our understanding of the computations performed.

This first chapter is devoted to the highest of Marr's levels, computational theory, and contains minor references to the lowest level, hardware implementation. It does not provide any new insight at these levels, but instead reviews important works from others that motivate the following chapters. The rest of the work lies squarely in the middle. We investigate the representations formed by an algorithm for producing codes of image data. We extend the algorithm, provide novel interpretations of its function, and make comparisons against alternative algorithms.

The research herein is interested in visual perception, and what sort of computational principles give rise to it. The actual substance of the thesis is gives no answers to this, but is a step along a particular line of inquiry that appears to be promising.

Comp vision had difficulty, why? They still struggle with core issues, like adversarial examples and catastrophic forgetting.

Marr - the brain as an information processor. Understanding the core computations of the brain. Although, he focused on single neuron point of view.

Barlow's redundancy reduction \& representation. Differentiating between the goals of the retina and the cortex. At each processing stage, the brain must form a representation that is both \textit{efficient} and \textit{useful}. Efficient is fairly easy to justify - there are constraints such as physical space and metabolic resources that must be adhered to. The idea of what might be useful is more vague, however. For example, when considering information representation in V1 - the utility of a representation is going to depend on the objective of downstream neurons, which is of course not known. V1 projects to a variety of brain areas, including [motion], [sensory integration], [memory], [ventral \& dorsal visual processing streams], etc, and therefore either the representation in V1 is general enough to be decoded in a useful way for many different tasks, or there are numerous independent representations in V1. Lets focus on information going to V2 and continuing along the dorsal pathway. If we agree with the "where" \& "what" hypotheses of the dorsal \& ventral pathways, then we would expect the representation in V1 that is being sent up the dorsal stream to reliably make explicit information that contributes to explaining what is in the world. This, combined with the efficiency constraint, presents us with a trade-off - in order to make some aspect of information explicit, one must make other aspects obscure [Marr]. We see examples of this readily in language, for example ...

Moving away from the single neuron doctrine. Reductionist approach moved the field to a position of idolizing the computations of an individual neuron, while disregarding the synergies created when computation is performed at the population level.


\section{Key observations of biological neurons in primary visual cortex}

\subsection{Inputs to V1}
[https://docs.google.com/presentation/d/1ADw8JSxwBAdpNfzK6EvsUHGf_DiqYJQXVVHyx0EusA0/edit#slide=id.g88dedb6d3_0_35]
To investigate V1, it is important to first understand how the signal that reaches V1 is encoded and what the many processes are that transform the incoming information. Below we have focused on three primary systems that modify the visual signal before it reaches V1: ocular motion, retinal sampling and processing, and the lateral geniculate nucleus (LGN). Our treatment of these systems is terse at best and there are more systems involved that each constitute an important research direction. None the less, it is important to highlight a few key details of these systems when considering how V1 processes information.

\subsubsection{Light} The process of light irradiating objects in the world before entering our eyes is \textit{highly} nonlinear. When light is incident on an object, a number of well described physical alterations occur. They vary dramatically based on the non-uniform physical composition of the object and include light scattering, absorptance, and reflectance. On top of this, objects typically move along a continuous nonlinear trajectory in the world and often occlude each other, resulting in a dramatic change in the incoming signal across occlusion boundaries. It is the goal of the vision system to deconstruct these effects to identify what is in the world. Luckily, most of these properties are consistent among objects and change smoothly through time. For example, the absorptance of a particular rock can be considered effectively constant and lions are highly unlikely to teleport. Thus, our brain can exploit the statistical structure of the input to decipher one object from another without explicitly modeling properties like absorptance. The first brain organelle in a long pipeline of visual processing stages is the retina.

\subsubsection{The retina} In order to see objects, our eyes focus reflected light onto our retinas. Each retina has over 100 million photoreceptors, which convert incident photons into chemical signals to be processed by the rest of the retina network. In most of the retina, multiple photoreceptors will converge onto a single output neuron, a ganglion cell. This convergence is not direct, however. The signal first passes through a complicated network of horizontal, bipolar, and amacrine cells that all do their part to transform and condition the signal. The number of photoreceptors to output channels is nearly a 1 to 1 ratio in the fovea, which, in humans, is a circular patch of retina where with the highest acuity, and it decreases to over 100 inputs to 1 output at the far periphery. As a result of this varying convergence, visual acuity is highest in the fovea, and decreases radially in the periphery. The photoreceptors also vary in the type of information they convey. About 120 million of them are rods, which primarily function in dim lighting conditions. About 6 to 7 million are cones, which supply most of our visual experience. In all, information from the roughly 130 million photoreceptors is conveyed to the brain by approximately 1.5 million ganglion cells. Mammalian ganglion cells themselves can be divided into at least 15 different types, most of which evenly tile the input space of the retina. These different output cell types convey different information to the brain about the same area in visual space. Although it is largely agreed upon that pre-processing is done by the retina before the signal is sent to the brain, the amount of processing and the method by which it is communicated is still highly debated. Work from JJ Atick and colleagues propose that the center-surround receptive field of the retinal ganglion cell performs a "whitening" of the input signal, which is a statistical transform that produces a signal with equal power at all spatial frequencies. Evidence for the retina performing spatial whitening of input signals has only been indirectly found through psychophysics experiments [atick, redlich 1992], although evidence for temporal whitening has been directly shown [dong, atick]. Whitening would be a desirable preprocessing step for the brain because it accounts for the largely uninformative and ubiquitous 1/f power statistics found in natural visual signals, which allows later processing stages to focus on higher order statistics that can more readily account for the causes of the incoming signal. Another important property of the retina is the division of labor among different cell types [van essen, anderson 1995]. Although there are many different ganglion cells, two primary types are the midget and parasol cells. They divide the visual input to encode low temporal, high spatial frequency (midget) and high temporal, low spatial frequency (parasol) signals. This division has recently been proposed as an optimal tiling of the space given the restrictions of the retina [ganguli] and the desire to compress the signal. It is also important to consider how this division of labor impacts processing downstream, where the magno (with parasol cell inputs) and parvo (with midget cell inputs) processing streams remain largely divergent through V1.

\subsubsection{Eye movements} In photography it is critically important to stabilize the camera, especially in dim lighting conditions. When the camera shutter is open, the light sensors continuously integrate light information such that any movement is going to result in blurring of spatial details. Many modern imaging systems have tools to compensate for this, but anyone who has tried to take a concert or city lights photo can tell you that it is far from a solved problem. Any organism that has eyes that move also has to solve this problem, but we know little about how it is done [Olshausen, Anderson 2010, Burak 2010]. Given the eccentrically nonuniform sampling being performed by the retina, a logical conclusion for the purpose of eye movements is that they are simply to “foveate”, or direct the fovea to, whatever object in the scene we wish to see with the most detail. This class of movements is called a saccade, and can range in scale from 1 degree visual angle to the full extent of your eye’s ability to move (>100 degrees). To put this into perspective, you can fit about 120 foveal cones (~2 microns in diameter each) into 1 degree visual angle on the retina (about 0.3mm). Another large scale eye movement is smooth pursuit, which has a unique time signature and involves actively and continuously adjusting fixation on an object that is moving in the world. Saccade and smooth pursuit eye movements are typically purposeful (or conscious) and are classified as non-fixational. Eye movements that subtend less than 1 degree visual angle are typically classified as fixational eye movements, are typically not purposeful, and are subdivided into microsaccades, drift, or tremor. During free viewing, microsaccades happen 1 to 2  times per second at a peak velocity of as much as 100 degrees per second. They have been shown to have much overlap with the physiological and psychophysical characteristics of larger-scale saccades. Drift, on the other hand, can be thought of as a low frequency (<40Hz), slower (25-60 arcmin/sec), constant-velocity fixational movement. As a first approximation, drift can be roughly characterized as following a Brownian path around a fixation point, in that the variance of the spatial distribution increases approximately linearly with time [Rucci, Victor, 2015]. However, this description of drift is merely to orient one to the nature of motion, for there lacks sufficient evidence that the motion itself is generated by a random process. Finally, tremor is typically characterized as a high-frequency (~70Hz), small amplitude random movement and is most often explained as noise caused by the muscles innervated on the eye. Fixational eye movements which were once all classified as noise, but now have been implicated in improving visual acuity and performing signal processing, namely normalization and whitening, on input data. Each class of eye movements, whether voluntary or involuntary, could potentially be constructively contributing to the signal that is interpreted by the retina. Considering that visual information translates across roughly 60 cones at a peak rate of about 6,000 cone diameters per second during a microsaccade, it is surprising that our perception is stable at all, much less improved. Recent efforts have established theories and demonstrations by experiment of how we might achieve stable perception [arathorn; bridgeman; murakami; burak] and how these fixational eye movements might be performing enhancing operations on the incoming visual information [rucci; ahissar; kenyon]. However, many still maintain that these eye movements are in fact a "bug" that must be compensated for [packer; kowler; engbert].

\subsubsection{The Lateral Geniculate Nucleus} After the incoming visual signal is transformed via eye movements and retinal processing, it is communicated along the optic tract to the lateral geniculate nucleus (LGN) of the thalamus and then to the primary visual cortex, V1. It has become increasingly clear that the thalamus performs considerable computations on the visual data, ...

\subsubsection{Accounting for these stages in our visual models} In our models of processing in V1, we largely ignore these important preprocessing stages. For natural scene stimulus, we will perform a "canonical preprocessing" step of whitening data to simulate the proposed whitening being performed by the retina and eye drift [Atick & Redlich 1992; Rucci & Victor 2015]. Beyond that, we are forced to assume that our model neurons are receiving input from a uniform sampling of identical phototransduction cells. Little work has been done to try to model the entire early vision pipeline, although [cottrell; lewicki; ganguli] provide promising directions. Integrating and reconciling their results with those herein is a compelling direction for future work.

\subsection{Early visual processing stages}
The mammalian primary visual cortex has a typical, stereotyped structure. 

\subsubsection{Pyramidal neurons}
A core computational device in V1 is the pyramidal neuron.

\subsubsection{Lateral circuits}
Lateral connectivity in the brain.

\subsubsection{Feedback, connectivity among layers}
Feedback in the brain. Subutai's stuff, Guillery \& Sherman.

\section{Theoretical foundation for understanding neural computation in V1}
We are interested in understanding how the early vision system transforms light signals into a neural code to provide information about objects in the world.
Contrary to what occurs in modern neural network models, the goal of our vision system is not to search a scene and label every object in it. It is also not intended to construct a veridical representation of our world, as many retina and visual cortex models attempt to accomplish. One theory (which we will subscribe to) is that the goal of the vision system is ecological. It aims to build a 3D representation of our world that integrates our many sensor modalities. This model must allow us to identify the causes of the incoming light signal, which includes labels, locations, poses, etc. of important objects in the scene. Our internal model must solve ill-posed problems to extract 3D scene information, integrate and store the information across time and modality, and guide motor actions. This is the groundwork upon which we seek to understand the computational principles of V1.


\subsection{Efficient coding}
Our visual world is a highly structured environment, with correlations and redundancies present at all spatial scales. Given restrictions in space, metabolism, and neural wiring length, it is in the best interest of an ideal visual observer to model these redundancies using an efficient code. When light first enters our eye, the initial objective of the vision system is to convey as much information as possible down the very limited bandwidth provided by the optic tract. Then, after this bottleneck, the brain has more freedom to adjust, modify, expand, or collapse the data signal, although the aforementioned constraints must still be considered. It appeals to our intuition to think that the brain should represent the world in a meaningful way, such that the causes of the scenes we see are encapsulated. Here I will discuss theories on how the brain efficiently models, or accounts for, redundancies in our world using a set of distributed, statistically independent representations.

Barlow, efficient coding.

\subsection{Predictive coding}
Predictive coding, relationship to EC.

\subsection{Hierarchical Bayesian inference}
Ultimately, we aim to build an hierarchical general representation of natural scenes. A hierarchical model of natural scenes should produce a general representation of input data that spans all layers. The bottom of the hierarchy contains information about the details of the scene, and as one ascends the hierarchy, one should see a more general, abstract description. Information moves up the hierarchy in the form of inference and down the hierarchy as expectations. Resolving ambiguities at the top are easier as there are more regularities and there is more context. These should help inform inference in lower layers to resolve ambiguities about minute details of the scene.

In the work of \citet{lee2003hierarchical}, they simplify the model by forcing each layer to only receive input from the layer below & above, as in a Markov chain. Expectations are propagated down to alter priors of lower layers in a dynamic, context-sensitive manner. Feedback connections influence the inference process, causing a layer to converge on an alternate solution that fits the expectations above. Although they use dynamic sampling algorithms to represent Bayesian inference, we believe it is possible to construct a version that is true to the theory and uses sparse coding as a core computational framework. We introduce some steps in that direction in chapters \ref{ch:applications} and \ref{ch:hierarchicalsc}.

\subsection{A probabilistic approach to image coding}

\subsection{Other important ideas}
Dynamic routing, HD computing. Not to be discussed further... 