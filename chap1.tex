\chapter{Introduction}\label{ch:intro}
The visual sense is powerful in that it typically dominates other sensory inputs for assessing the physical properties of objects \parencite{ernst2002humans}. However, our conscious perception of the world does not match the visual signal coming into our eyes. The dichotomy between the noisy light signal reaching our brain and the immediate and compelling nature of our visual percept can bias our investigations \parencite{rodieck1998first}. Vision is intuitive for humans, which makes it difficult to grasp the complicated nature of deriving our perception from light reflections in the world. Indeed, early machine vision work approached the problem from what is in hindsight a simplistic perspective \parencite{papert1966summer}. For example, the seemingly elementary process of differentiating edges caused from physical boundaries of objects against those caused by lighting (i.e. shadows) was once considered introductory by the computational vision community, but it is still unsolved today \parencite{adelson2000lightness, barron2012shape}. Much of the history of studying vision has been dominated by the notion that it is simply a passive sense facilitated by filters and feature detectors, although experimental evidence overwhelming supports an alternative viewpoint that vision is an active, dynamic process \parencite{olshausen201320}. Vision research is fraught with inherent complications, unsolved problems, and unintuitive answers, but it is also the source of compelling examples of fascinating phenomena in human perception and consciousness. It is motivated by our ability to perceive over 10 orders of magnitude of illuminance levels \parencite{norton2002psychophysical}, untangle complicated inverse graphics problems such as shape from shading and edge detection, and rapidly recognize objects. This all happens while also demonstrating extreme robustness to input noise and distributional shift. It is also motivated by instances where we fail to perceive the stimulus as it is physically, in the form of illusions, bi-stable percepts, and metamers. Importantly, all of these phenomena are accessible and compelling to anyone. They drive the field at large and the work in this thesis.

It is well understood that there exists a dichotomy between how we perceive our world and the actual information impinging our sensory systems. For example, our eyes are constantly in motion, even when we fixate our gaze upon a stationary object, and yet the fixated object appears stable. Another example is that the retina unevenly samples visual information, yet we perceive a uniformly resolved visual world. Questions then arise about what sort of signal processing the brain is performing to allow us to perceive the world as we do or, more narrowly, how processing in the brain results in ecologically relevant features and objects becoming salient. Early works in philosophy identified this and drew ties between objective laws of nature, our perceptual experience, and consciousness \parencite{kant1790critique, helmholtz1878facts} (and see \parencite{westheimer2008helmholtz} for an excellent retrospective). One can make a clear connection from the ideas espoused in these and other early works to a general movement in the theoretical neuroscience community to think of perception as an ``inference'' problem. By this we mean that the brain uses prior knowledge of the world, gained through evolution or experience or most likely both, to resolve ambiguity in sensory signals, resulting in perception. This line of reasoning has been propagated by contemporary vision scientists and mathematicians, including Joseph Atick \citeyearpar{atick1990towards}, Horace Barlow \citeyearpar{barlow2001redundancy}, Fred Attneave \citeyearpar{attneave1954some}, David Field \citeyearpar{field1994goal}, David Mumford \citeyearpar{mumford1994pattern}, Bruno Olshausen \citeyearpar{olshausen2013perception} and others. Early philosophy suggested that perception is a \textit{guided hallucination}, while today it is depicted as an \textit{active process}. This is to say that our brain composes what we perceive, using information from the outside world as an anchor.


\section{Modeling the brain as an information processor}
Neuroscience is a discipline that spans many levels of understanding in order to investigate how the nervous system works. The field is broad, so before continuing it is important to narrow the scope. In David Marr's 1982 book, \textit{Vision} \citeyearpar{marr1982vision}, he defines three levels of understanding that since have been used heavily to motivate theoretical neuroscience research. These levels are Computational Theory, Representation and Algorithm, and Hardware Implementation. Importantly, Marr asserts that all three levels must be understood before one can emulate the entity under study. In our case, we wish to understand how brains work and how to build a brain. The answers to these questions are vast and largely unsolved, but here we provide a contribution. We will investigate hypotheses for computations underlying visual perception by analyzing models of neural coding and signal representation.

Marr also supported the perspective of thinking about the brain as an information processor. This is an idea that many theoretical neuroscience academics take for granted, but was not considered seriously until relatively recently. The seminal early works of Hubel and Weisel \citeyearpar{hubel1959receptive} cemented the idea for many when they showed that individual visual neurons responded selectively to relevant features in the world. From this and other works, and likely resulting from our limited ability to record from populations of neurons, many in the community have adopted a ``single neuron doctrine'', whereby experiments and explanations of phenomena are based in the information processing capabilities of the individual neuron. Indeed, many attempts at understanding non-linear response properties of visual neurons are focused on individual responses \parencite{priebe2012mechanisms}, but this reductionist approach moved the field to a position of idolizing the computations of an individual neuron while disregarding the synergies created when computation is performed at the population level. Several modern works have advised that the community should move away from this perspective \parencite{barlow1972single, olshausen1999probabilistic, zetzsche1999atoms, series2003silent} and towards the idea of population coding in the brain, which requires the consideration of the information processing capabilities of a family of neurons in a given region. A complete understanding of neural computation will likely require both population level and individual neuron level analysis. This principle underlies the core models, experiments, and theories studied in this thesis.

At each processing stage, the brain must form a representation that is both \textit{efficient} and \textit{useful}. Efficient is fairly easy to explain - there are constraints such as physical space and metabolic resources that must be adhered to. However, the idea of what might be useful is more vague. For example, consider information representation in the primary visual cortex (V1) - the utility of a representation is going to depend on the objective of downstream neurons, which is not fully known. V1 projects to a variety of brain areas, including those responsible for self motion, sensory integration, memory, and the rest of the visual processing hierarchy. Therefore, either the representation in V1 needs to be general enough to be decoded for many different tasks, or there must be numerous independent representations in V1. Indeed, to prescribe the entire visual processing stream to a single objective now seems foolish \parencite{barlow2001redundancy}, although this has been the predominant paradigm in neural computation. The primary model we study is focused on neural coding in V1. It is not described at a biophysical level of analysis and is not meant to represent a universal computational principle for visual processing. Instead, it is a tractable implementation of a general idea that information must be represented in V1 at a population level, with an emphasis on making explicit important details in the signal for later brain regions to interpret while preserving as much information as possible. Success of the model should provide support for this idea and motivate further work into alternate levels of analysis such as a hardware implementation (i.e. how might the brain actually do this). Before going into the V1 model, we will spend the rest of this chapter outlining the inputs to V1 and important theoretical motivation for the model itself.


\section{Inputs to V1}
%REF: [https://docs.google.com/presentation/d/1ADw8JSxwBAdpNfzK6EvsUHGf_DiqYJQXVVHyx0EusA0/edit#slide=id.g88dedb6d3_0_35]
To investigate V1, it is important to first understand how the signal that reaches V1 is encoded and what the many processes are that transform the incoming information. Below we have focused on three primary systems that modify the visual signal before it reaches V1: ocular motion, retinal sampling and processing, and the thalamus. Our treatment of these systems is terse at best, and there are more systems involved that each constitute an important research direction. Nonetheless, it is important to highlight a few key details of these systems when considering how V1 processes information.


\subsubsection{Light}
The process of light irradiating objects in the world before entering our eyes is \textit{highly} nonlinear. When light is incident on an object, a number of well described physical alterations occur. They vary dramatically based on the non-uniform physical composition of the object and include light scattering, absorptance, and reflectance. On top of this, objects typically move along a continuous nonlinear trajectory in the world and often occlude each other, resulting in a dramatic change in the incoming signal across occlusion boundaries. It is the goal of the vision system to deconstruct these effects in order to identify what is in the world. Luckily, most of these properties are consistent among objects and change smoothly through time. For example, the absorptance of a particular rock can be considered effectively constant and objects in the world are do not teleport from one position to another. Thus, our brain can exploit the stability of the input to decipher one object from another without explicitly modeling physical properties like absorptance. This is accomplished via a series of anatomically localized stages that all share recurrence as a common processing motif. The first stage in the visual processing hierarchy is the retina.


\subsubsection{The retina}
In order to see objects, our eyes focus reflected light onto our retinas.
Each retina has over 100 million photoreceptors, which convert incident photons into chemical signals to be processed by the rest of the retinal network.
Photoreceptors vary in the type of information they convey.
About 120 million of them are rods, which primarily function in dim lighting conditions and do not convey color information.
The other 6 to 7 million of them are cones, which supply most of our visual experience.
In most of the retina, multiple photoreceptors will converge onto a single output neuron, a ganglion cell.
This convergence is not direct, however.
The signal first passes through a complicated network of horizontal, bipolar, and amacrine cells that all do their part to transform and condition the signal.
The ratio of photoreceptors to output channels is nearly 1 to 1 in the fovea, which, in humans, is a circular patch of retina with the highest acuity.
The ratio decreases to over 100 inputs to 1 output at the far periphery.
As a result of this varying convergence, visual acuity is highest in the fovea, and decreases radially in the periphery.
In all, information from the roughly 130 million photoreceptors is conveyed to the brain by approximately 1.5 million ganglion cells.
Mammalian ganglion cells themselves can be divided into at least 15 different types, most of which evenly tile the input space of the retina.
These different output cell types convey different information to the brain about the same area in visual space. Finally, information is sent from the retina down the optic tract to the brain. The optic tract is a narrow communication channel with important constraints that are relevant to information coding: one is that the diameter of the nerve bundle creates a ``blindspot'' where your retina cannot process light information, and another is that the eye needs to be able to move quickly, so the bundle must be flexible enough to allow this movement. Both of these constraints encourage the retina to prioritize compressing information to be passed down a narrow channel to the brain.

Although it is largely agreed upon that preprocessing is done by the retina before the signal is sent to the brain, the amount of processing and the method by which it is communicated is still highly debated.
Work from Joseph Atick and colleagues proposes that the center-surround receptive field of the retinal ganglion cell performs a ``whitening'' of the input signal, which is a statistical transform that produces a signal with equal power at all spatial frequencies \parencite{atick1990towards, atick1992what}.
Evidence for the retina performing spatial whitening of input signals has only been indirectly found through psychophysics experiments \parencite{atick1992what}, although evidence for temporal whitening has been directly shown \parencite{dong1995statistics} by recording from thalamic inputs.
Whitening would be a desirable preprocessing step for the brain because it performs decorrelation on the structured input signal, which allows for a more efficient use of the optic tract.

Another important property of the retina is the division of labor among different cell types \parencite{van1995information}.
Although there are many different ganglion cells, two primary types are the midget and parasol cells.
They divide the visual input to encode low temporal, high spatial frequency (midget) and high temporal, low spatial frequency (parasol) signals.
This division has been proposed as an optimal division of the input given the restrictions of the retina and the desire to compress the signal \parencite{mcintosh2016deep, van1995information}.
It is also important to consider how this division of labor impacts processing downstream, where the magno (with parasol cell inputs) and parvo (with midget cell inputs) processing streams remain largely divergent through V1.

\subsubsection{Eye movements}
In photography it is critically important to stabilize the camera, especially in dim lighting conditions.
When the camera shutter is open, the light sensors continuously integrate light information such that any movement is going to result in blurring of spatial details.
Many modern imaging systems have tools to compensate for this, but anyone who has tried to take a concert or city lights photo can tell you that it is far from a solved problem.
Any organism with eyes also has to solve this problem, but we know little about how it is done \parencite{olshausen2010does, burak2010bayesian}.

Given the eccentrically nonuniform sampling being performed by the retina, a logical conclusion for the purpose of eye movements is that they are simply to ``foveate'', or direct the fovea to, whatever object in the scene we wish to see with the most detail.
This class of movements is called a saccade, and can range in scale from 1 degree visual angle to the full extent of your eyesâ€™ ability to move (\textgreater 100 degrees).
To put this into perspective, you can fit about 120 foveal cones ($\sim$2 microns in diameter each) into 1 degree visual angle on the retina (about 0.3mm).
Another large scale eye movement is smooth pursuit, which has a unique (near constant velocity) time signature and involves actively and continuously adjusting fixation on an object that is moving in the world.
Saccade and smooth pursuit eye movements are typically purposeful (or conscious) and are classified as non-fixational.
Eye movements that subtend less than 1 degree visual angle are classified as fixational eye movements, are typically not conscious, and are subdivided into microsaccades, drift, or tremor.
During free viewing, microsaccades happen 1 to 2 times per second at a peak velocity of as much as 100 degrees per second.
They have been shown to have much overlap with the physiological and psychophysical characteristics of larger-scale saccades.
Drift, on the other hand, can be thought of as a low frequency (\textless 40Hz), slower (25-60 arcmin/sec), and constant-velocity fixational movement.
As a first approximation, drift can be roughly characterized as following a Brownian path around a fixation point, in that the variance of the spatial distribution increases approximately linearly with time \parencite{rucci2015unsteady}.
However, this description of drift is merely to orient one to the nature of motion, for there is not sufficient evidence that the motion itself is generated by a random process.
Finally, tremor is typically characterized as a high-frequency ($\sim$70Hz), small amplitude random movement and is most often explained as noise caused by the muscles innervated on the eye.

Fixational eye movements were once all classified as noise, but now have been implicated in improving visual acuity \parencite{ratnam2017benefits, rucci2007miniature} and performing signal processing functions, such as normalization and whitening \parencite{aytekin2014visual}.
Each class of eye movements, whether voluntary or involuntary, could potentially be constructively contributing to the signal that is interpreted by the retina.
Considering that visual information translates across roughly 60 cones at a peak rate of about 6,000 cone diameters per second during a microsaccade, it is surprising that our perception is stable at all, much less improved.
Recent efforts have established theories and demonstrations by experiment of how we might achieve stable perception \parencite{arathorn2013unstable, bridgeman2010brain, murakami1998jitter, burak2010bayesian} and how these fixational eye movements might be performing enhancing operations on the incoming visual information \parencite{ahissar2012seeing, mostofi2016visual, kenyon2004correlated}.
However, many still maintain that these eye movements are in fact a ``bug'' that must be compensated for \parencite{packer1992blurring, kowler1979miniature, engbert2011integrated}.

In our work we assume the retinal input is fixed, although there is active research underway that includes eye motion \parencite{ratnam2017benefits, anderson2019high}. Incorporating eye motion into models of V1 computation will involve extending them to explicitly code for time-varying stimulus, an example of which can be found in \parencite{olshausen2003learning}. Understanding how eye motion influences the signal coming to the eye will undoubtedly be extremely important for understanding neural computation in V1. 


\subsubsection{The Thalamus}
After the incoming visual signal is transformed via eye movements and retinal processing, it is communicated along the optic tract to the lateral geniculate nucleus (LGN) of the thalamus and then to the primary visual cortex, V1. It has become increasingly clear that the thalamus performs considerable computations on the visual data. Like their retinal inputs, LGN neurons are selective to onset and offset of light, and have structured spatiotemporal receptive fields. Unlike the retina, however, the LGN has binocular and color-opponent cells \parencite{schiller1978functional, schmielau1977role}. It is known that the division of labor identified as different processing streams (most notably the magno- and parvo-cellular streams) is maintained in the LGN and there are notable differences between cells in these streams. For example, conductance velocity (which likely impacts information transfer rate and temporal sensitivity) for magnocellular inputs is faster than parvocellular and although both streams exhibit center-surround organization, the magnocellular neurons appear to be invariant to wavelength while the parvocellular neurons have specific color opponency \parencite{schiller1978functional}. Although many regard the LGN as a relay station for information traveling from retina to cortex, the retina accounts for only a small fraction of the inputs to the LGN \parencite{weyand2016multifunctional}. Other inputs include branched afferents from axons that ultimately connect to motor centers as well as inputs from the brainstem, suggesting that information coming from the thalamus includes motor instructions \parencite{guillery2002thalamic}. Additional feedback connectivity from the cortex likely provides context information that allows for modulation of the feedforward signal \parencite{weyand2016multifunctional, ghodrati2017towards}. The pulvinar nucleus of the thalamus has been proposed as a driver for higher order cortical areas as well, where information from cortical areas is routed through the thalamus before driving other cortical areas. Indeed, anatomical and physiological evidence suggests that layer 5 (output) cortical neurons drive thalamus, instead of just modulating it, and that thalamic outputs provide powerful inputs to higher-order visual cortical regions \parencite{guillery2002thalamic}. The thalamus can perform significant modification to the signal coming into the cortex: it can be used to adjust binocular salience \parencite{schmielau1977role}, dynamically route or gate information \parencite{olshausen1993neurobiological, weyand2016multifunctional}, and compensate for motor commands \parencite{guillery2011branched}.


\subsubsection{Accounting for these stages in our visual models}
The focus of this thesis is in understanding a particular model for information processing in V1, namely sparse coding. Our analysis of the model provides important explanations of shared response properties between the model and biological systems. We also generate hypotheses about coding properties of the model. However, the burden of motivating this model as a candidate for brain computation is largely left to previous and future work. As such, much less emphasis is put into our models having high correspondence to the biological input (pre-V1) pathway of animals in the real world. In the following chapters, we will reduce all of the processing in early vision areas to simple linear transforms. Instead, our focus is on understanding the model dynamics themselves. We will use static images of natural scenes, as well as small contrived datasets as stimulus for our model. Extending the algorithms and ideas from this thesis to account for the complexity of preprocessing in the retina and thalamus, as well as inputs from other important visual and non-visual cortical regions would be an important contribution to the field. We do believe that including these important facets of biological processing will not detract from the core computational motivations or principles in the sparse coding model.

For natural scene stimulus, we will perform a preprocessing step of whitening data  \citeyearpar{olshausen1997sparse} to simulate the proposed whitening being performed by the eye \parencite{atick1992what, rucci2015unsteady}.
Beyond that, we assume that our model neurons are receiving input from a uniform sampling of identical phototransduction cells.
Little work has been done to try to model the entire early vision pipeline, although \parencite{shan2013efficient, doi2007theory, lindsey2019unified} all provide promising directions.
With this in mind, it is important to also note that the core goal of the sparse coding model is not to prescribe a specific computational algorithm to V1 cells, but instead to determine what is possible by applying an efficiency-constrained generative model to natural data.
This broad concept is proposed to be an underlying computational principle in V1, which has been well supported in the literature.

%\section{Selected features of the visual cortical hierarchy}
%
%
%\subsection{Early visual processing stages}
%The mammalian primary visual cortex has a typical, stereotyped structure. 
%
%
%\subsubsection{Pyramidal neurons}
%A core computational device in V1 is the pyramidal neuron.
%
%
%\subsubsection{Lateral circuits}
% Nearly every circuit in the visual processing pipeline has inhibitory lateral connectivity, from retina \parencite{rordiak} to V1 \cite{xu2016primary}. What is the functional role of this connectivity? In retina it is described as a normalization circuit. stuff h2 horizontal cells? I have this somewhere... Horizontal connections are also implicated as a means of achieving divisive normalization (heeger, simoncelli). They provide a central role in the sparse coding model described in the coming chapters. Talk about Adesnik & Heider stuff.
%
%
%\subsubsection{Feedback, connectivity among layers}
%Feedback in the brain. Subutai's stuff, Guillery \& Sherman.


\section{Theoretical foundation for understanding neural computation in V1}\label{sec:ch1_theory}
We are interested in understanding how the early vision system transforms light signals into a neural code to provide information about objects in the world.
Contrary to what occurs in modern neural network models, we believe that the goal of our vision system is not to search a scene and label every object in it. The goal of the vision system is more ecological. It aims to build a representation of our world that integrates our many sensor modalities. It allows us to identify the causes of the incoming light signal, which includes labels, locations, poses, etc. of important objects in the scene. Our internal model must account for ill-posed problems to extract 3D scene information, integrate and store the information across time and modality, and guide motor actions. This is the groundwork upon which we seek to understand the computational principles of V1.

Our visual world is a highly structured environment, with correlations and redundancies present at all spatial scales. Given restrictions in space, metabolism, and neural wiring length, it is in the best interest of an ideal visual observer to model these redundancies using an efficient code. The initial objective of the vision system is to convey as much light information as possible from the eye to the brain using the limited bandwidth provided by the optic tract. Then, after this bottleneck, the brain has more freedom to adjust, modify, expand, or collapse the data signal, although the aforementioned biophysical constraints must still be considered. It appeals to our intuition to think that the brain should represent the world in a meaningful way, such that the causes of the scenes we see are encapsulated. Here we will discuss theories on how the brain efficiently models, or accounts for, redundancies in our world using a set of distributed, statistically independent representations.

Fred Attneave \citeyearpar{attneave1954some} was the first to apply newly minted ideas in information theory to understanding visual coding. He suggested that efficiency was a primary goal of the visual system. The efficient coding hypothesis was later posed in terms of redundancy reduction \parencite{barlow1961possible}, with reasoning that the brain may seek an efficient code of the input in order to minimize the number of neurons required to represent the signal. This theory was supported by \parencite{laughlin1981simple}, who compared the responses of fly eyes with contrast levels in natural scenes and suggested that the coding mechanism use is optimal in terms of efficiency and the neuron's information capacity. Joseph Atick \citeyearpar{atick1992could} suggested that information theory could provide a basis for understanding sensory processing, which was also supported from other findings around the same time \parencite{hateren1992theory, field1994goal}.

Anatomical evidence tells us that V1 expands the image representation coming from LGN by having many more outputs than inputs \parencite{olshausen2003principles}. Thus, redundancies are probably \textit{created} in the perceptual process. The goal of cortical processing, then, cannot be said to be strictly redundancy reduction or compression. As an alternative to redundancy reduction, several researchers have argued that the goal of perception should not be discussed in isolation from action; an organism forms perceptual representations for the purpose of directing its behavior towards the achievement of desirable outcomes and away from undesirable ones \parencite{barlow2001redundancy, simoncelli2001natural}. From this perspective, the brain aims to extract the statistical structure of the input in order to form a``meaningful'' representation that recovers the environmental causes of the sensory data, which it can use to guide action. Along these lines, the efficient coding hypothesis has been revised to emphasize redundancy \textit{representation} rather than reduction \parencite{barlow2001redundancy}. Redundancies in the input signal indicate structure in the environment. An encoding that makes these redundancies explicit encodes the causal and statistical structure of the environment, which the organism can exploit to plan and direct behavior. The first-order method for doing this is to disregard all details that are not pertinent to ecologically relevant objects in the world, which happens to be precisely what is done in current object recognition deep networks \parencite{tishby2015deep}. However, a better solution is to learn the probabilistic structure of the world, such that one can \textit{generate} the fine details when needed. In the words of Barlow: ``Instead of thinking of neural representations as transformations of stimulus energies, we should regard them as approximate estimates of the probable truths of hypotheses about the current environment, for these are the quantities required by a probabilistic brain working on Bayesian principles'' \parencite{barlow2001redundancy}. This notion of a probabilistic brain is shared by many \parencite{kersten2004object, lee2003hierarchical, lewicki1997bayesian, olshausen2013perception}, and is a core principle underlying the research in this thesis.

% TODO: add content from Tolhurst (1992); Ruderman & Bialek (1994) 
To apply the ideas outlined above to vision, we need to understand the statistics of natural images.
David Field used Fourier analysis to demonstrate that the power spectra of natural images typically falls off as $\tfrac{1}{f^{2}}$, where $f$ is spatial frequency and the computed power is circularly averaged across orientations \parencite{field1987relations}.
Wherever there is signal, as measured by Fourier power, then there is structure in the scene.
Therefore, a $\tfrac{1}{f^{2}}$ power spectrum tells us that we should expect to find structure at all (reasonable) spatial frequencies in natural scenes, which is hopefully intuitive if you consider the high spatial frequencies present in tree bark or sand and the low spatial frequencies found from faint shadows on a wall or clouds in the sky.
However, this characteristic ``pink'' power spectrum is not sufficient for generating natural scenes.
Natural scenes also exhibit a large amount of higher-order structure, such as elongated edges.
A dictionary of wavelets can be used to efficiently encode such structure \parencite{field1999wavelets}.
They produce more independent codes than other dictionaries, which results in a sparse code with high kurtosis\footnote{Kurtosis is the fourth-order moment of a data population, which indicates ``peakiness'' by measuring how much mass lies in the tails of the distribution. We would want our code to be peaked at zero, indicating that most of the neurons are silent.}.
Perhaps unsurprisingly, the linear approximation of receptive fields for individual V1 neurons are well-fit with a localized wavelet dictionary composed of Gabor functions.
The notion of efficiency as measured by sparsity will is a core component of the sparse coding model, which is our primary subject of interest.

It has been demonstrated that a typical redundancy reducing code leads to large errors in estimates of the frequency of a particular input, since many neurons are active in response to both the input of interest as well as other stimuli \parencite{gardnermedwin2001limits}.
A sparse code, on the other hand, learns dictionary elements that occur independently in the environment and produces a factorial code.
Any deviations from a factorial distribution signal indicate a previously unknown statistical dependency to be learned.
Later, we will discuss how hierarchical extensions of sparse coding can model these deviations and learn higher order statistical regularities.
An ultimate goal of these extensions is to develop a model that extracts all of the statistical structure to form a \textit{meaningful} representation that recovers the environmental causes of the sensory data to guide action.


\section{Thesis outline}\label{sec:ch1_outline}
This thesis investigates how lateral connectivity and recurrent inference affects image coding properties of model neurons. The primary network of interest is rooted in probabilistic inference and builds an internal model of its sensory world through experience. Although the algorithm is fairly simple, its lineage goes back to a centuries old idea that our \textit{perception} of the world is achieved via an active, conscious process rooted in exploiting the unique statistical structure of natural signals. This first chapter was devoted to outlining important biological systems that lead up to the brain region of interest - V1. Additionally, we introduced relevant theories from others that motivate the later chapters. Next, We will describe the sparse coding model and Locally Competitive Algorithm (LCA) in detail, with a complete derivation and exposition of the features learned and coding properties. We then extend the model for a semi-supervised learning application as well as a hierarchical variant that learns cells that are analogous to ``complex'' cells in V1. Finally, we improve upon recent methods for understanding model neurons, which allows us to increase a network's degree of nonlinearity without significantly decreasing our understanding of the computations performed. We use this method to argue for increasing the complexity of model neurons in neuroscience and machine learning research and to provide a perspective on the robustness, efficiency, and selectivity/invariance properties of LCA.