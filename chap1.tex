\chapter{Introduction}
The visual sense is powerful in that it typically dominates cross-modality inputs for assessing physical properties of objects \cite{ernst2002humans}. However, our perception of the world does not match the visual signal coming into the eye. The dichotomy between the noisy light signal reaching our brain and the immediate and compelling nature of our visual percept can bias our investigations \cite{rodieck1998first}. Vision is intuitive for humans, which makes it difficult to grasp the complicated nature of deriving our perception from light reflections in the world. Indeed, early machine vision work approached the problem from what is in hindsight a simplistic perspective \cite{papert1966summer}. For example, the seemingly elementary process of differentiating edges caused from physical boundaries of objects against those caused by lighting (i.e. shadows) was once considered introductory by the computational vision community but is still unsolved today \cite{adelson2000lightness, barron2012shape}. Much of the history of studying vision has been dominated by the notion that it is simply a passive sense facilitated by filters and feature detectors \cite{olshausen201320}. Vision research is fraught inherent complications, unsolved problems, and unintuitive answers, but it is also the source of compelling examples of fascinating phenomena in human perception and consciousness. It is motivated by our ability to perceive over 10 orders of magnitude of illuminance levels \cite{norton2002psychophysical}, untangle complicated inverse graphics problems such as shape from shading and edge detection, rapidly recognize objects and yet also demonstrate extreme robustness to input noise and distributional shift. It is also motivated by instances where we fail to perceive the stimulus as it is physically, in the form of illusions, bi-stable percepts, and metamers. Importantly, all of these phenomena are accessible and compelling to anyone. They drive the field at large and the work herein.

It is well understood that there exists a dichotomy between how we perceive our world and the actual information impinging our sensory systems. For example, our eyes are constantly in motion, even when we fixate our gaze upon a stationary object, and yet the fixated object appears stable. The retina itself uses uneven sampling of visual information, yet we perceive a uniformly resolved visual world. Questions then arise about what sort of signal processing is the brain performing to allow us to perceive the world as we do, or more narrowly what processing results in making ecologically relevant features and objects salient. Early works in philosophy identified this dichotomy, and drew ties between objective laws of nature, our perceptual experience, and consciousness \cite{kant1790critique, helmholtz1878facts} (and see \cite{westheimer2008helmholtz} for an excellent retrospective). Once can make a clear connection from the ideas espoused in these and other early works to a general movement in the theoretical neuroscience community to think of perception as an ``inference'' problem. By this we mean that the brain uses prior knowledge of the world, gained through evolution or experience or most likely both, to resolve ambiguity in sensory signals, resulting in perception. This line of reasoning has been propagated by contemporary vision scientists and mathematicians, including Horace Barlow \citeyearpar{barlow2001redundancy} and David Mumford \citeyearpar{mumford1994pattern}. Early philosophy suggested that perception is a \textit{guided hallucination}, while today it is depicted as an \textit{active process}. This is to say that our brain composes what we perceive using information from the outside world as an anchor.


\section{Choosing a level of analysis}
Neuroscience is a discipline that spans many levels of understanding in order to investigate how the nervous system works. The field is broad, so before continuing it is important to narrow the scope. In David Marr's 1982 book, \textit{Vision} \citeyearpar{marr1982vision}, he defines three levels of understanding that since have been used heavily to motivate theoretical neuroscience research. These levels are Computational Theory, Representation and Algorithm, and Hardware Implementation. Importantly, Marr asserts that all three levels must be understood before one can emulate the entity under study. In our case, we wish to understand how brains work and how to build a brain. The answers to these questions are vast, but here we provide a contribution. We will investigate the computations underlying visual perception by analyzing models of neural coding and signal representation.

Marr also supported the perspective of thinking about the brain as an information processor. This is an idea that many theoretical neuroscience students take for granted, but was not considered seriously until relatively recently. The seminal early works of Hubel & Weisel \citeyearpar{hubel1959receptive} cemented the idea for many when they showed that individual visual neurons responded selectively to relevant features in the world. From this and other works, and likely resulting from our limited ability to record from populations of neurons, many in the community have adopted a ``single neuron doctrine'', whereby experiments and explanations of phenomena are based in the information processing capabilities of the individual neuron. Indeed, many attempts at understanding non-linear response properties of visual neurons are focused on individual responses \cite{priebe2012mechanisms}. This reductionist approach moved the field to a position of idolizing the computations of an individual neuron, while disregarding the synergies created when computation is performed at the population level. However, several modern works have advised that the community moves away from this perspective \cite{barlow1972single, olshausen1999probabilistic, zetzsche1999atoms, series2003silent} and towards an idea of population coding in the brain, which requires considering the information processing capabilities of a family of neurons in a given region. This principle underlies the core models and theories in this thesis.

At each processing stage, the brain must form a representation that is both \textit{efficient} and \textit{useful}. Efficient is fairly easy to justify - there are constraints such as physical space and metabolic resources that must be adhered to. The idea of what might be useful is more vague, however. For example, consider information representation in the primary visual cortex, or V1 - the utility of a representation is going to depend on the objective of downstream neurons, which is of course not known. V1 projects to a variety of brain areas, including [motion], [sensory integration], [memory], [ventral \& dorsal visual processing streams], etc, and therefore either the representation in V1 need be general enough to be decoded for many different tasks, or there are numerous independent representations in V1. Indeed, to prescribe the entire visual processing stream to a single objective now seems foolish \cite{barlow2001redundancy}, although this has been the predominant paradigm in neural computation. The models herein are focused on coding in the V1. The model is not described at a biophysical level of analysis, and is not meant to represent a universal computational principle for visual processing. Instead, it is a tractable implementation of a general idea that information must be represented in V1 at a population level, with an emphasis on making explicit important details in the signal for later brain regions to interpret while preserving as much information as possible. Before going into the V1 model, we will spend the rest of this chapter outlining the inputs to V1 and important theoretical motivation for the model itself.


\section{Inputs to V1}
%REF: [https://docs.google.com/presentation/d/1ADw8JSxwBAdpNfzK6EvsUHGf_DiqYJQXVVHyx0EusA0/edit#slide=id.g88dedb6d3_0_35]
To investigate V1, it is important to first understand how the signal that reaches V1 is encoded and what the many processes are that transform the incoming information. Below we have focused on three primary systems that modify the visual signal before it reaches V1: ocular motion, retinal sampling and processing, and the lateral geniculate nucleus (LGN). Our treatment of these systems is terse at best and there are more systems involved that each constitute an important research direction. None the less, it is important to highlight a few key details of these systems when considering how V1 processes information.


\subsubsection{Light}
The process of light irradiating objects in the world before entering our eyes is \textit{highly} nonlinear. When light is incident on an object, a number of well described physical alterations occur. They vary dramatically based on the non-uniform physical composition of the object and include light scattering, absorptance, and reflectance. On top of this, objects typically move along a continuous nonlinear trajectory in the world and often occlude each other, resulting in a dramatic change in the incoming signal across occlusion boundaries. It is the goal of the vision system to deconstruct these effects to identify what is in the world. Luckily, most of these properties are consistent among objects and change smoothly through time. For example, the absorptance of a particular rock can be considered effectively constant and objects in the world are highly unlikely to teleport. Thus, our brain can exploit the statistical structure of the input to decipher one object from another without explicitly modeling physical properties like absorptance. The first brain organelle in the long pipeline of visual processing stages is the retina.


\subsubsection{The retina}
In order to see objects, our eyes focus reflected light onto our retinas. Each retina has over 100 million photoreceptors, which convert incident photons into chemical signals to be processed by the rest of the retina network. Photoreceptors vary in the type of information they convey. About 120 million of them are rods, which primarily function in dim lighting conditions. About 6 to 7 million are cones, which supply most of our visual experience. In most of the retina, multiple photoreceptors will converge onto a single output neuron, a ganglion cell. This convergence is not direct, however. The signal first passes through a complicated network of horizontal, bipolar, and amacrine cells that all do their part to transform and condition the signal. The number of photoreceptors to output channels is nearly a 1 to 1 ratio in the fovea, which, in humans, is a circular patch of retina where with the highest acuity. The ratio decreases to over 100 inputs to 1 output at the far periphery. As a result of this varying convergence, visual acuity is highest in the fovea, and decreases radially in the periphery. In all, information from the roughly 130 million photoreceptors is conveyed to the brain by approximately 1.5 million ganglion cells. Mammalian ganglion cells themselves can be divided into at least 15 different types, most of which evenly tile the input space of the retina. These different output cell types convey different information to the brain about the same area in visual space. Although it is largely agreed upon that pre-processing is done by the retina before the signal is sent to the brain, the amount of processing and the method by which it is communicated is still highly debated. Work from Joseph Atick and colleagues propose that the center-surround receptive field of the retinal ganglion cell performs a ``whitening'' of the input signal, which is a statistical transform that produces a signal with equal power at all spatial frequencies \cite{atick1990towards, atick1992what} . Evidence for the retina performing spatial whitening of input signals has only been indirectly found through psychophysics experiments \cite{atick1992what}, although evidence for temporal whitening has been directly shown \cite{dong1995statistics}. Whitening would be a desirable preprocessing step for the brain because it accounts for the largely uninformative and ubiquitous $1/f$ power statistics found in natural visual signals (see \cite{field1999wavelets}, \cite{field1989statistics}, and section \ref{ch1:theory} for more on this), which allows later processing stages to focus on higher order statistics that can more readily account for the causes of the incoming signal. Another important property of the retina is the division of labor among different cell types \cite{van1995information}. Although there are many different ganglion cells, two primary types are the midget and parasol cells. They divide the visual input to encode low temporal, high spatial frequency (midget) and high temporal, low spatial frequency (parasol) signals. This division has recently been proposed as an optimal tiling of the space given the restrictions of the retina \cite{mcintosh2016deep} and the desire to compress the signal. It is also important to consider how this division of labor impacts processing downstream, where the magno (with parasol cell inputs) and parvo (with midget cell inputs) processing streams remain largely divergent through V1.

\subsubsection{Eye movements}
In photography it is critically important to stabilize the camera, especially in dim lighting conditions. When the camera shutter is open, the light sensors continuously integrate light information such that any movement is going to result in blurring of spatial details. Many modern imaging systems have tools to compensate for this, but anyone who has tried to take a concert or city lights photo can tell you that it is far from a solved problem. Any organism that has eyes that move also has to solve this problem, but we know little about how it is done \cite{olshausen2010does, burak2010bayesian}. Given the eccentrically nonuniform sampling being performed by the retina, a logical conclusion for the purpose of eye movements is that they are simply to “foveate”, or direct the fovea to, whatever object in the scene we wish to see with the most detail. This class of movements is called a saccade, and can range in scale from 1 degree visual angle to the full extent of your eye’s ability to move (>100 degrees). To put this into perspective, you can fit about 120 foveal cones (~2 microns in diameter each) into 1 degree visual angle on the retina (about 0.3mm). Another large scale eye movement is smooth pursuit, which has a unique (near constant velocity) time signature and involves actively and continuously adjusting fixation on an object that is moving in the world. Saccade and smooth pursuit eye movements are typically purposeful (or conscious) and are classified as non-fixational. Eye movements that subtend less than 1 degree visual angle are classified as fixational eye movements, are typically not purposeful, and are subdivided into microsaccades, drift, or tremor. During free viewing, microsaccades happen 1 to 2  times per second at a peak velocity of as much as 100 degrees per second. They have been shown to have much overlap with the physiological and psychophysical characteristics of larger-scale saccades. Drift, on the other hand, can be thought of as a low frequency (<40Hz), slower (25-60 arcmin/sec), constant-velocity fixational movement. As a first approximation, drift can be roughly characterized as following a Brownian path around a fixation point, in that the variance of the spatial distribution increases approximately linearly with time \cite{rucci2015unsteady}. However, this description of drift is merely to orient one to the nature of motion, for there lacks sufficient evidence that the motion itself is generated by a random process. Finally, tremor is typically characterized as a high-frequency (~70Hz), small amplitude random movement and is most often explained as noise caused by the muscles innervated on the eye. Fixational eye movements were once all classified as noise, but now have been implicated in improving visual acuity \cite{ratnam2017benefits, rucci2007miniature} and performing signal processing, such as normalization and whitening \cite{aytekin2014visual}. Each class of eye movements, whether voluntary or involuntary, could potentially be constructively contributing to the signal that is interpreted by the retina. Considering that visual information translates across roughly 60 cones at a peak rate of about 6,000 cone diameters per second during a microsaccade, it is surprising that our perception is stable at all, much less improved. Recent efforts have established theories and demonstrations by experiment of how we might achieve stable perception \cite{arathorn2013unstable, bridgeman2010brain, murakami1998jitter, burak2010bayesian} and how these fixational eye movements might be performing enhancing operations on the incoming visual information \cite{ahissar2012seeing, mostofi2016visual, kenyon2004correlated}. However, many still maintain that these eye movements are in fact a ``bug'' that must be compensated for \cite{packer1992blurring, kowler1979miniature, engbert2011integrated}. In our work we assume the retinal input is fixed, although understanding how eye motion influences the signal coming to the eye will undoubtedly be extremely important for understanding neural computation in V1.


\subsubsection{The Thalamus}
After the incoming visual signal is transformed via eye movements and retinal processing, it is communicated along the optic tract to the lateral geniculate nucleus (LGN) of the thalamus and then to the primary visual cortex, V1. It has become increasingly clear that the thalamus performs considerable computations on the visual data. Like their retinal inputs, LGN neurons are selective to onset and offset of light, and have structured spatiotemporal receptive fields. Unlike the retina, however, the LGN has binocular and color-opponent cells \cite{schiller1978functional, schmielau1977role}. It is known that the division of labor identified as different processing streams (most notably the magno- and parvo-cellular stream) is maintained in the LGN and there are notable differences between cells in these streams. For example, conductance velocity (which likely impacts information transfer rate and temporal sensitivity) for magnocellular inputs is faster than parvocellular and although both streams exhibit center-surround organization, the magnocellular neurons appear to be invariant to wavelength while the parvocellular neurons have specific color opponency \cite{schiller1978functional}. Although many regard the LGN as a relay station for information traveling from retina to cortex, the retina accounts for only a small fraction of the inputs to the LGN \cite{weyand2016multifunctional}. Other inputs include branched afferents from axons that ultimately connect to motor centers as well as inputs from the brainstem, suggesting that information coming from the thalamus includes motor instructions \cite{guillery2002thalamic}. Additional feedback connectivity from the cortex likely provides context information that allows for modulation of the feedforward signal \cite{weyand2016multifunctional, ghodrati2017towards}. The pulvinar nucleus of the thalamus has been proposed as a driver for higher order cortical areas as well, where information from cortical areas is routed through the thalamus before driving other cortical areas. Indeed, anatomical and physiological evidence suggests that layer 5 (output) cortical neurons drive thalamus, instead of just modulating it and that thalamic outputs provide powerful inputs to higher-order visual cortical regions \cite{guillery2002thalamic}. The thalamus can perform significant modification to the signal coming into the cortex: it can be used to adjust binocular salience \cite{schmielau1977role}, dynamically route or gate information \cite{olshausen1993neurobiological, weyand2016multifunctional}, and compensate for motor commands \cite{guillery2011branched}.


\subsubsection{Accounting for these stages in our visual models}
The focus of this thesis is in understanding a particular model for information processing in V1, namely sparse coding. Our analysis of the model provides important explanations of shared response properties between the model and biological systems and we generate hypotheses about coding properties of the model. However, the burden of motivating this model as a candidate for brain computation is largely left to previous work. As such, much less emphasis is put into modeling and accounting for biological realism along the input pathway of animals in the real world. In the following chapters, we will reduce much of the processing in early vision areas to simple transforms. We will use static images of natural scenes, as well as small contrived datasets as stimulus for our model, with a focus on understanding the model dynamics themselves. Extending the algorithms and ideas herein to account for the complexity of preprocessing in the retina, thalamus, as well as inputs from other important visual and non-visual cortical regions would be an important contribution to the field. We do believe that including these important facets of biological processing will not detract from the core computational motivations or principles in the sparse coding model.

For natural scene stimulus, we will perform a ``canonical preprocessing'' step of whitening data to simulate the proposed whitening being performed by the retina and eye drift \cite{atick1992what, rucci2015unsteady}. Beyond that, we assume that our model neurons are receiving input from a uniform sampling of identical phototransduction cells. Little work has been done to try to model the entire early vision pipeline, although \cite{shan2013efficient, doi2007theory, lindsey2019unified} provide promising directions. With this in mind, it is important to also note that the core goal of the sparse coding model is not to prescribe a specific computational algorithm to V1 cells, but instead to determine what is possible by applying an efficiency-constrained generative model to natural data. This broad concept is proposed to be an underlying computational principle in V1, which has been well supported in the literature.

%\section{Selected features of the visual cortical hierarchy}
%
%
%\subsection{Early visual processing stages}
%The mammalian primary visual cortex has a typical, stereotyped structure. 
%
%
%\subsubsection{Pyramidal neurons}
%A core computational device in V1 is the pyramidal neuron.
%
%
%\subsubsection{Lateral circuits}
%Lateral connectivity in the brain. Xu et al 2016 primary
%
%
%\subsubsection{Feedback, connectivity among layers}
%Feedback in the brain. Subutai's stuff, Guillery \& Sherman.


\section{Theoretical foundation for understanding neural computation in V1}
We are interested in understanding how the early vision system transforms light signals into a neural code to provide information about objects in the world.
Contrary to what occurs in modern neural network models, the goal of our vision system is not to search a scene and label every object in it. It is also not intended to construct a veridical representation of our world, as many visual cortex models attempt to accomplish. The goal of the vision system is ecological. It aims to build a representation of our world that integrates our many sensor modalities. This model must allow us to identify the causes of the incoming light signal, which includes labels, locations, poses, etc. of important objects in the scene. Our internal model must solve ill-posed problems to extract 3D scene information, integrate and store the information across time and modality, and guide motor actions. This is the groundwork upon which we seek to understand the computational principles of V1.

Our visual world is a highly structured environment, with correlations and redundancies present at all spatial scales. Given restrictions in space, metabolism, and neural wiring length, it is in the best interest of an ideal visual observer to model these redundancies using an efficient code. When light first enters our eye, the initial objective of the vision system is to convey as much information as possible down the very limited bandwidth provided by the optic tract. Then, after this bottleneck, the brain has more freedom to adjust, modify, expand, or collapse the data signal, although the aforementioned constraints must still be considered. It appeals to our intuition to think that the brain should represent the world in a meaningful way, such that the causes of the scenes we see are encapsulated. Here we will discuss theories on how the brain efficiently models, or accounts for, redundancies in our world using a set of distributed, statistically independent representations. Horace Barlow has published a number of documents supporting the broad notion of ''efficient coding`` as a normative goal of the visual system. Specifically, he states that important aspects of visual scenes must be represented efficiently in the brain \cite{barlow2001redundancy}. The first order method for doing this is to disregard all details that are not pertinent to ecologically relevant objects in the world, which happens to be precisely what is done in current object recognition deep networks [tishby, 2015]. However, a better solution is to learn the probabilistic structure of the world, such that one can \textit{generate} the fine details when needed. In the words of Barlow: ``Instead of thinking of neural representations as transformations of stimulus energies, we should regard them as approximate estimates of the probable truths of hypotheses about the current environment, for these are the quantities required by a probabilistic brain working on Bayesian principles'' \cite{barlow2001redundancy}. This notion of a probabilistic brain is shared by many [kersten2003object, lee2003hierarchical, lewicki1997bayesian, olshausen2013perception], and is a core principle underlying the research herein.

Images of natural scenes have a stereotyped structure. David Field used Fourier analysis to demonstrate that the power spectra of natural images typically falls off as $\tfrac{1}{f^{2}}$, where $f$ is the circularly averaged (across orientation) spatial frequency \cite{field1987relations}. However, this characteristic ''pink`` power spectrum is not sufficient for generating natural scenes. Natural scenes also exhibit a large amount of higher-order structure, such as elongated edges. A dictionary of wavelets can be used to efficiently encode such structure \cite{field1999wavelets}. They produce more independent codes than other dictionaries, which results in a sparse code with high kurtosis (the forth order moment of a data population, which indicates how much mass lies in the tails of the distribution). Perhaps unsurprisingly, the linear approximation of receptive fields for individual V1 neurons are well fit with a localized wavelet dictionary composed of Gabor functions. The notion of efficiency as measured by sparsity will be a primary motivator for the sparse coding model that is the primary point of interest in this thesis.

\section{Thesis outline}
This thesis investigates how lateral connectivity and recurrent inference affects image coding properties of model neurons. The primary model of interest is rooted in probabilistic inference and builds an internal model of its sensory world through experience. We improve upon recent methods for understanding model neurons, which allows us to increase the degree of nonlinearity without significantly decreasing our understanding of the computations performed. We use this method to argue for increasing the complexity of model neurons in neuroscience and machine learning research. We also utilize these methods to provide a perspective on the robustness, efficiency, and selectivity/invariance properties of model neurons. We extend the model using ideas of hierarchical feedback to a semi-supervised learning application space as well as a hierarchical learning model.

This first chapter was devoted to outlining important biological systems that lead up to the brain region of interest - V1. Additionally, we introduced important theories from others that motivate the later chapters. The second chapter describes in detail an algorithm for producing sparse codes of image data and makes comparisons to other popular models of visual neural computation. We then extend an existing method for probing the response geometry of model neurons to provide novel interpretations and predictions of the algorithm. Finally, we extend the algorithm to novel versions that include important concepts of hierarchical coding in the brain's visual processing pipeline.
