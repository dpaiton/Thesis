\chapter{Models for Biological Encoding of Natural Images}\label{ch:lca}

\section{Introduction}
Sparse coding is a generative model for explaining natural signals. In other words, it models the joint probability distribution between natural signals and a latent code such that it is able to generate new naturalistic signals from the code. Although the model and motivating theory have been explored in other modalities, here we will focus on coding visual stimulus as a proxy for computations performed by layer 4 simple cells in V1. We will begin this chapter with a probabilistic derivation for the sparse coding model. Importantly, this will make clear the various assumptions built into the model, which will be relevant for our interpretations of results in later chapters. The first section will get us to an energy function that a sparse coding algorithm would seek to minimize. In the second section, we recapitulate the sparse coding model from an optimization perspective. Next, we derive the Locally Competitive Algorithm (LCA), which is a neural network model designed to perform sparse inference. This network is the subject of the rest of the thesis. Finally, we compare the LCA to other popular neurally-inspired image coding models, namely Predictive Coding \parencite{rao1999predictive} and ICA \parencite{bell1997independent}. Depending on the context, we may refer to the connections between pixels and neurons as a matrix of weight vectors, a dictionary of elements, or a set of basis functions - these should be considered synonymous in this work.

\section{A probabilistic approach for image coding}
As was explained in chapter 1, the signal coming from the two-dimensional retina is incomplete and the problem of inferring the exact three-dimensional structure of the world from that signal is impossible. % TODO: Fix chapter ref
However, some solutions are more likely than others and ecological pressure encourages brains to perceive highly likely solutions.
If population activity in the brain represents possible hypotheses about the world, then we want to model the brain in a probabilistic, Bayesian framework.
In this section, we derive such a framework and show that sparse coding approximately solves it.

We aim to encode an incoming signal efficiently under the assumption that the signal is composed of structured components and unstructured additive noise. We do this with sparse coding, which assumes a linear generative model:

\begin{equation} \label{eq:ch2_sparse_generative_model}
    s = \Phi a + \varepsilon,
\end{equation}

\noindent where $s$ is our input signal, $\Phi$ is an overcomplete (in that the number of columns in $\Phi$ is greater than the number of rows) dictionary of functions, $a$ is a row vector of activation coefficients, and $\varepsilon$ is Gaussian noise (which we justify below). For a given input signal, we must solve the ill-posed task of inferring an optimal set of coefficients. The problem is difficult in part because the noise is unknown and therefore cannot be completely accounted for. Another reason is that the dictionary is overcomplete and thus non-orthogonal, resulting in an infinite number of possible combinations that are each equally valid representations. Following the works of Olshausen and colleagues \parencite{olshausen1996learning, olshausen2003principles, karklin1999porbabilistic}, we will derive a probabilistic generative model for computing image codes. Additionally, we can follow a similar derivation to learn a dictionary that maximizes coding efficiency. This gives us a principled and extendable foundation from which we can derive sparse coding and similar models.


\subsection{Inference}
Our model assumes a prior probability over the activity coefficients, $p(a)$. We also assume that the coefficient prior does not depend on the dictionary itself; that is $p(a|\Phi) = p(a)$. Although in practice the coefficients and dictionary will be jointly optimized, we can adhere to this last assumption by fixing one variable while optimizing the other in an expectation-maximization-like procedure. We specify the likelihood of an image under the model for a given state of coefficients as $p(s|a,\Phi)$. Conversely, given evidence in the form of an image sample, we can define the posterior as the conditional probability distribution of our coefficients given the image and model: $p(a|s,\Phi)$. Our goal is to maximize this posterior distribution, which gives us the most probable estimate of the coefficients. As is done in \parencite{karklin1999porbabilistic}, we can relate the likelihood and posterior using Bayes' rule:

\begin{equation}\label{eq:ch2_bayes}
    p(a|s,\Phi) \propto p(s|a,\Phi) p(a),
\end{equation}

\noindent where we use $\propto$ to indicate that we are ignoring the image probability because it does not affect inference or learning. We model the noise distribution as Gaussian, which we justify by assuming that the generative model can account for everything except for random, uncorrelated (i.e. independent) causes that combine in the limit to follow a Gaussian distribution. Under this assumption, we can specify the log probability density function (up to additive constants) for our image likelihood:

\begin{equation}\label{eq:ch2_image_likelihood}
    \log p(s|a,\Phi) = -\frac{1}{2\sigma_{\text{n}}^{2}}\|s-\phi a\|^{2} + c,
\end{equation}

\noindent where $\sigma_{\text{n}}$ is the standard deviation of the additive noise. We will later drop the additive constants, $c$, as they do not effect inference or learning. Our goal is to encode the images as efficiently as possible. To do this, we impose a factorial (i.e. independent) prior on the coefficients that is highly peaked at 0, which encourages most of the neurons to be silent for any given input signal. We assume that our activations, $a$, are independent and positive-only with finite mean. The maximum entropy distribution for such a random variable is a one-sided Laplacian, with log probability density

\begin{equation}\label{eq:ch2_coefficient_prior}
    \log p(a) = -\lambda\sum_{i}|a_{i}| + c,
\end{equation}

\noindent where $\lambda$ is the inverse of the mean. Now that we have densities for our log probabilities, we can write down the log-likelihood of our model up to additive constants, $L$, which we will later define as the negative of the sparse coding energy function:

\begin{align}\label{eq:ch2_log_likelihood}
\begin{split}
    L &= \log\left(p(s|a,\Phi)p(a)\right) \\
      &= \log{p(s|a,\Phi)} + \log{p(a}) \\
      &= -\frac{1}{2\sigma_{\text{n}}^{2}}\|s - \Phi a\|^{2} - \lambda \sum_{i}|a_{i}|.
\end{split}
\end{align}

As we said earlier, we wish to maximize the posterior distribution to infer a sparse code. That is, we wish to do a maximum a-posteriori (MAP) estimate. To do this we can find a minimum of the negative log-likelihood:

\begin{align}\label{eq:ch2_min_log_likelihood}
\begin{split}
    \hat{a} = \max_{a} p(a|s,\Phi) &= \max_{a}\left[\log{p(s|a,\Phi)} + \log{p(a)}\right] \\
                         &= \min_{a}\frac{1}{2\sigma_{\text{n}}^{2}}\|s - \Phi a\|^{2} + \lambda \sum_{i}|a_{i}|.
\end{split}
\end{align}

\subsection{Learning}
The probability of the image under the model, $p(s|\Phi)$, is obtained by marginalizing over the activations:

\begin{equation}\label{eq:ch2_image_probability}
    p(s|\Phi) = \int p(s|a,\Phi) p(a) da.
\end{equation}

To find the optimal dictionary, we have to maximize the model posterior. This is equal to the image likelihood times the weight prior and tells us how good our model is given some data samples:

\begin{equation}\label{eq:ch2_dictionary_posterior}
  p(\Phi|s) \propto p(s|\Phi)p(\Phi).
\end{equation}

We impose an $l_{2}$ prior on the pixel dimension of the dictionary, which we write down as a log density:

\begin{equation}\label{eq:ch2_dictionary_prior}
    \log p(\Phi_{i}) = -\frac{1}{2\sigma_{\Phi}^{2}}\sum_{i}\|\Phi_{i}\|^{2} + c,
\end{equation}

\noindent where $\sigma_{\Phi}^{2}$ is proportional the expected pixel variance of the individual weight vectors and the sum penalizes each weight individually. We've now defined our entire log posterior:

\begin{equation}\label{eq:ch2_log_probability_weight_complete}
    \log{p(\Phi|s)} = \log p(\Phi) + \log \int p(s|a,\Phi)p(a)da,
\end{equation}

\noindent where the integral tells us that we want to do inference. Ideally, we would like to maintain a distribution of probable coefficients for a given image and model. However, to do this exactly is intractable and so is sampling from the coefficients to approximate the integral. Instead, we will again use a MAP estimate, which is a good approximation if the model is sure of the posterior. That is to say that $p(a|s,\Phi)$ truly follows a highly peaked distribution with heavy tails (i.e. the distribution has high kurtosis). This gives us

\begin{align}\label{eq:ch2_log_probability_weight_approx}
\begin{split}
    \log{p(\Phi|s)} &= \log p(\Phi) + \log \int p(s|a,\Phi)p(a)da \\
    &\approx \log p(s|\hat{a},\Phi)+ \log p(\hat{a}) + \log p(\Phi)  \\
    &= -\frac{1}{2\sigma_{\text{n}}^{2}}\|s-\Phi \hat{a}\|^{2} - \lambda \sum_{i}|\hat{a}_{i}| - \frac{1}{2\sigma_{\Phi}^{2}}\sum_{i}\|\Phi_{i}\|^{2},
\end{split}
\end{align}

\noindent where $\hat{a}$, as above, is a MAP estimate of the posterior. Now we can take the gradient with respect to our model to get the weight update:

\begin{equation}\label{eq:ch2_log_probability_weight_grad}
    \nabla_{\Phi} \log{p(\Phi|s)} &\approx \frac{1}{\sigma_{\text{n}}^{2}}[s-\Phi \hat{a}]\hat{a}^{\top} - \frac{1}{\sigma_{\Phi}^{2}}\|\Phi\|
\end{equation}

As we will see in the following section, the learning and inference rules can be derived directly from the same energy function, which is the negative of equation \eqref{eq:ch2_log_likelihood}. In practice, instead of imposing a prior on the weights, we will force them to have unit $l_{2}$ norm by normalizing them after each weight update. This ends up being equivalent to doing projected gradient descent on the energy function. From this point we can make a simple change to the inference rule to define the LCA. Finally, we will discuss some important variants to the LCA, analyze the network trained on natural images, and derive comparisons to similar models.


\section{Sparse coding}
In practice, sparse coding on images involves the tasks of learning a dictionary from data and, given a dictionary, finding an optimal sparse code for an input signal. We define an optimal sparse code as one that gives the most faithful representation of the data using the fewest dictionary elements. The primary objective of sparse coding is to encode input data (e.g. an image of a natural scene) in a generative framework, such that the data can be reconstructed from the code \parencite{olshausen1997sparse}. We want the encoding to be of a higher dimensionality than the input image; that is to say we want our dictionary to be overcomplete.

For an $N$-dimensional input signal (e.g. an $N$ pixel image), $s \in \mathbb{R}^{N}$, we want a dictionary of $M > N$ vectors, where each vector is of length $N$. In other words, the dictionary is a matrix $\Phi$ with dimensions $N \times M$. Each of the $M$ columns are dictionary elements, $\Phi_{i}, i \in \{1,...,M\}$, and each dictionary element has a corresponding coefficient, $a_{i}, i \in \{1,...,M\}$, that we can assemble into a vector of activations. Our goal is to minimize an energy function that requires both a faithful and efficient representation of the data, in that it reconstructs the data well with the fewest number of active elements. We can express this mathematically as

\begin{equation}\label{eq:ch2_sparse_energy}
    \argmin\limits_{a, \Phi}
        \left( E =
            \overbrace{ \tfrac{1}{2} \| s - \hat{s} \|_{2}^{2} }^\text{Preserve Information} +
        \overbrace{ \lambda \sum\limits_{i=1}^{M}C(a_{i}) }^\text{Limit Activations} \right),
\end{equation}

\noindent where $\hat{s} = \sum\limits_{i=1}^{M}\Phi_{i}a_{i}$ is the image reconstruction, $\argmin\limits_{a, \Phi}$ tells us that we want to minimize the energy with respect to both the activations and the dictionary, and $||\cdot||_2^2$ indicates the squared $l_2$ norm. In this energy function, we use $\lambda$ as a trade-off parameter between the reconstruction quality and the sparsity constraint. Note that we have performed a change of variables from the previous section, to include the noise variance, $\sigma_{\text{n}}$ into a new $\lambda$ parameter that now modulates the $l_{1}$ cost as a Lagrange multiplier. In practice we tune it to maximize network sparsity for a minimum accepted reconstruction quality. One popular choice for the cost function, $C(\cdot)$, is the $l_{1}$ cost penalty, which is the sum of the absolute value of all activations:

\begin{equation}\label{eq:ch2_l1_cost}
  \sum\limits_{i=1}^{M}C(a_{i}) = \sum\limits_{i=1}^{M}|a_{i}|.
\end{equation}

Depending on the choice of cost function, solving equation \eqref{eq:ch2_sparse_energy} can be convex or non-convex. For the $l_{1}$ cost, it is convex. Previous work has solved this via alternating projected gradient descent \parencite{olshausen1997sparse} or basis pursuit denoising \parencite{chen2001atomic}. The basis pursuit denoising variant is often accomplished using the iterative shrinkage and thresholding algorithm (ISTA) \parencite{daubechies2004iterative, beck2009fast}. Rozell et al. \citeyearpar{rozell2008sparse} proposed an alternative to ISTA using a neural network following dynamics governed by the energy gradient called the Locally Competitive Algorithm (LCA). The following section explains this alternative in detail. It is also possible to consider an $l_{0}$ like cost, which measures the total number of neurons active and is non-convex. This can be approximately solved by matching pursuit \parencite{davis1997adaptive, rehn2007network, rebollo2002optimized}, or with a particular ``hard thresholded'' variant of the LCA (see section \ref{sec:ch2_lca_properties}).


\section{The Locally Competitive Algorithm}\label{sec:ch2_lca}
The Locally Competitive Algorithm (LCA) is a method for computing a sparse code from a given input signal and dictionary and was original described by Christopher J. Rozell and colleagues \citeyearpar{rozell2008sparse}. The model describes an activation coefficient, $a_{k}$, as the thresholded output of some model neuron's internal state, $u_{k}$, which is analogous to the neuron's membrane potential. This internal state follows similar dynamics to a leaky integrator neuron model. The LCA can be described as a continuous system that is implementable in hardware, or as a discrete system with an internal state that advances as a function of a time step. If implemented in hardware, you can follow current dynamics (Kirchhoff's law for an equivalent RC circuit) to perform inference extremely quickly \parencite{rozell2008sparse}. As the model state advances, the system relaxes to a minimum solution of the energy function described in equation \eqref{eq:ch2_sparse_energy}. Figure \ref{fig:ch2_lca_diagram} gives a schematic drawing of the LCA network. In this section we will derive the dynamical equation for computing the state transitions from the sparse coding energy function.

\begin{figure}[h]
    \centering %center using container as reference, instead of the whole text
    \includegraphics[width=0.4\textwidth]{figures/lca_diagram.png}
    \caption{\textbf{LCA architecture.} The LCA network includes feedforward driving connections, $\Phi$ as well as a lateral connectivity matrix, $G$. The input, $s$, is a vector of pixels, which is the lower row of black dots. The neurons are the upper row of black dots and have internal states, $u$. The dark arrows are connections for neuron $k$. All other connections are denoted by dotted lines.}
    \label{fig:ch2_lca_diagram}
\end{figure}

We wish for our neuron activations to minimize the energy function described in equation \eqref{eq:ch2_sparse_energy}. This will be accomplished via gradient descent. To illustrate the derivative, we first express the energy function in subscript notation:

\begin{equation}\label{eq:ch2_subscript_sparse_energy_func}
    E(t) = \tfrac{1}{2} \sum\limits_{i=1}^{N} \left[ s_{i} - \sum\limits_{j=1}^{M}a_{j}(t) \Phi_{i,j} \right]^{2} + \lambda \sum\limits_{j=1}^{M} C(a_{j}(t))
\end{equation}

and then we take the derivative with respect to an individual neuron's activity, $a_{k}(t)$. Since we ultimately want to do gradient \textit{descent}, we will write the negative derivative:

\begin{align}\label{eq:ch2_lca_deda_extended}
\begin{split}
    - \frac{\partial E(t)}{\partial a_{k}(t)}
    =
        &\sum\limits_{i=1}^{N} S_{i} \Phi_{i,k} -
        \Phi_{i,k}\sum\limits_{j=1}^{M}a_{j}(t) \Phi_{i,j} -
        \lambda \sum\limits_{j=1}^{M}\frac{\partial C(a_{j}(t))}{\partial a_{k}(t)} \\
    =
        &\sum\limits_{i=1}^{N} \left[ s_{i} \Phi_{i,k} -
        \sum\limits_{j \neq k}^{M} \Phi_{i,k} \Phi_{i,j} a_{j}(t) - \Phi_{i,k}\Phi_{i,k}a_{k}(t) \right] -
        \lambda \frac{\partial C(a_{k}(t))}{\partial a_{k}(t)}
\end{split}
\end{align}

We constrain the model to have unit norm dictionary elements in the pixel dimension, $||\phi_{i}||_2^2 = 1$. This means that the $\sum_{i=1}^{N}\Phi_{i,k}\Phi_{i,k}a_{k}(t)$ term in equation \eqref{eq:ch2_lca_deda_extended} reduces to $a_k(t)$:

\begin{equation}\label{eq:ch2_lca_deda}
    -\frac{\partial E(t)}{\partial a_{k}(t)} =
    \sum\limits_{i=1}^{N} \left[ s_{i} \Phi_{i,k} -
    \sum\limits_{j \neq k}^{M} \Phi_{i,k} \Phi_{i,j} a_{j}(t) \right] - a_{k}(t) -
    \lambda \frac{\partial C(a_{k}(t))}{\partial a_{k}(t)}
\end{equation}

$E(t)$ and $a(t)$ both vary in time, but in future sections we will drop the time variable to improve the equation readability. In this scenario, we can imagine a constant input signal, $s$, and a constant set of dictionary elements, $\Phi$. Given these constants, we want the system to evolve over time to produce an optimal set of activations, $a$. Equation \eqref{eq:ch2_subscript_sparse_energy_func} gives us the energy at each time point during this inference process. Each neuron has a corresponding dictionary element, $\phi_{k}$, which is a vector that indicates the connection strength to each pixel in the input. In the literature this is also referred to as the strength of the synaptic connections between the neuron and each input pixel, or the neuron's feed-forward receptive field. In this model, we are going to find a sparse code for each individual image patch. Additionally, all $M$ neurons will be connected to the same image patch, $s$ (i.e. the model is ``fully-connected''). The model neuron has a driving excitatory input, which we will denote $b_{k} = S\phi_{k} = \sum_{i=1}^{N}s_{i} \Phi_{i,k}$. This is the projection of the input image onto the neuron's corresponding dictionary element. The stronger the similarity between the input, $s$, and the dictionary element, $\phi_{k}$, the stronger the driving excitatory input. Each neuron also receives inhibition from every other active neuron. The magnitude of inhibition is partially set by an $M \times M$ Gramian matrix, $G$. The matrix evaluates to the inner product of the each neuron's dictionary element with all other neurons' dictionary elements, $G = \Phi^T\Phi$, such that each element in $G$ gives the overlap in pixel space between two dictionary elements, $G_{i,j} = \sum\limits_{n=1}^{N} \Phi_{n,i}\Phi_{n,j}$. The total inhibition onto a neuron is also scaled by how active the inhibiting neuron is.

Next, we define a new function that maps the activity for a neuron and the sparsity cost function to a scalar. This function can be thought of as the self inhibition that increases as the value of $a_{k}$ increases:

\begin{equation}\label{eq:ch2_hopfield_t_func}
  f_{\lambda}(a_{k}(t)) = a_{k}(t) + \lambda \frac{\partial C(a_{k}(t))}{\partial a_{k}(t)}
\end{equation}

Self inhibition imposes sparsity by penalizing the neurons own output, which is different from the sparsity inducing input from other laterally connected neurons. Now we can write the partial derivative of our energy function (equation \eqref{eq:ch2_subscript_sparse_energy_func}) in terms of our new variables:

\begin{equation}\label{eq:ch2_lca_deda_simple}
    - \frac{\partial E(t)}{\partial a_{k}(t)} =
    b_{k} -
    \sum\limits_{j \neq k}^{M} G_{k,j} a_{j}(t) -
    f_{\lambda}(a_{k}(t)).
\end{equation}

At this point we could update $a(t)$ using gradient descent following equation \eqref{eq:ch2_lca_deda_simple} to produce a sparse code from an input signal. However, a more biologically consistent solution would be to have the model neuron maintain an internal state, analogous to a biological neuron's membrane potential. The model neuron could then only produce output when its membrane potential exceeded some threshold. This has better correspondence to biology and it gives the neurons sub-threshold dynamics that influences their activity (see section \ref{sec:ch2_lca_properties}) when compared to directly using equation \eqref{eq:ch2_lca_deda_simple}. Following this logic, Rozell et al. \citeyearpar{rozell2008sparse} define an internal state variable, $u_{k}(t)$ that represents the membrane potential for neuron $k$ at time $t$. When a neuron's potential climbs above some threshold, it communicates in the form of an activation, $a_{k}(t)$, which is analogous to a spike rate\footnote{Neuroscientists often use the spike rate, or number of spikes per fixed unit of time, as a measure of the overall activity of a neuron. The LCA is not a spiking network, but when comparing to biological neurons (e.g. in the work by \cite{zhu2013visual} and in section \ref{sec:ch4_selectivity_efficiency}) we make a direct analogy between the activity of the LCA neuron and the biological neuron's spike rate. Spiking versions of sparse coding have been explored by \parencite{zylberberg2011sparse, olshausen2003learning}.}. Only these excited neurons can contribute to the image code and reconstruction. Ultimately, the network of neurons should still descend the energy function from equation \eqref{eq:ch2_subscript_sparse_energy_func}, so we define the neuron's state dynamics to be governed by the following equation:

\begin{align}\label{eq:ch2_u_dot}
\begin{split}
    \dot{u_{k}}(t) &\propto - \frac{\partial E(t)} {\partial a_{k}(t)} \\
    \dot{u_{k}}(t) &= \frac{1}{\tau} \left[b_{k} - \sum_{m \neq k}^{M}G_{k,m}a_{m}(t) - f_{\lambda}(a_{k}(t)) \right],
\end{split}
\end{align}

\noindent where $\tau$ is a proportionality constant that represents the time constant of the dynamics.

In order to have a complete description of the model, we need to describe a relationship between $u$ and $a$. Earlier the $f_{\lambda}(a_{k}(t))$ term was described as a self inhibition term that encourages sparsity. Another way to enforce self inhibition is to introduce a membrane leak term. If we assign the internal state, $u_{k}(t)$, to this function:

\begin{equation}\label{eq:ch2_u_func_a}
    u_k(t) = f_{\lambda}(a_{k}(t)),
\end{equation}

\noindent then we can think of our neuron as a leaky integrator. We can also invert the function to get our neuron's output activity:

\begin{displaymath}\label{eq:ch2_a_fu_thresh}
    a_{k}(t) = f_{\lambda}^{-1}(u_{k}(t)) := T_{\lambda}(u_{k}(t)).
\end{displaymath}

This gives us the LCA neuron update equation:
\begin{equation}\label{eq:ch2_u_dot_full}
    \dot{u_{k}}(t) = \frac{1}{\tau} \left[\overbrace{b_{k}}^\text{Driving excitation} - \overbrace{\sum_{m \neq k}^{M}G_{k,m}a_{m}(t)}^\text{Lateral Inhibition} - \overbrace{u_{k}(t)}^\text{Leak} \right],
\end{equation}

\noindent or equivalently

\begin{displaymath}
    \tau \dot{u_{k}}(t) + u_{k}(t) =  b_{k} - \sum_{m \neq k}^{M}G_{k,m}a_{m}(t).
\end{displaymath}

When the neuron's membrane potential passes over a threshold, defined by $T_{\lambda}(u_{k}(t)) = f_{\lambda}^{-1}(u_{k})$, it becomes active:

\begin{equation}\label{eq:ch2_a_thresh}
  a_{k}(t) = T_{\lambda}(u_{k}(t)).
\end{equation}

For this expression to perform gradient descent on the energy function, $a$ must be a monotonically increasing function of $u$. Rozell et al. \citeyearpar{rozell2008sparse} describe the relationship between the sparseness cost penalty, the neuron activity, and the internal membrane potential via a thresholding function. The thresholding function can take various forms that determine the exact nature of the sparseness penalty. For the $l_{1}$ penalty, Rozell et al. \citeyearpar{rozell2008sparse} use a soft thresholding function:

\begin{equation}\label{eq:ch2_lca_threshold_func}
    T_{\lambda}(u_{k}(t)) = \left\{
    \begin{aligned}
        u_{k}(t)+\lambda,\;\; &u_{k}(t)\; <\; -\lambda \\
        0,\;\; &-\lambda < u_{k}(t)\; <\; \lambda \\
        u_{k}(t)-\lambda,\;\; &u_{k}(t)\; >\; \lambda
    \end{aligned}
    \right.
\end{equation}

Here $\lambda$ indicates the sparseness penalty trade-off as well as the threshold that the membrane potential must surpass for the neuron to become active. An illustration of how one gets to the thresholding function from the $l_{1}$ penalty is given in figure \ref{fig:ch2_lca_thresh}. With the internal state dynamics from equation \eqref{eq:ch2_u_dot_full} and the thresholding function in equation \eqref{eq:ch2_lca_threshold_func}, we have defined a network that settles to a sparse code, $a$, which represents the input signal. For all of the work in this thesis, we will diverge from the implementation in \parencite{rozell2008sparse} and implement a rectified version of the thresholding function. That is, from equation \eqref{eq:ch2_lca_threshold_func}, the first term for when $u_{k}(t) < -\lambda$ is set to $T_{\lambda}(u_{k}(t))=0$. This adds a higher degree of nonlinearity, and is important for implementing hierarchical sparse coding models.

\begin{figure}[h]
    \centering %center using container as reference, instead of the whole text
    \includegraphics[width=0.4\textwidth]{lca_cost_graphs.png}
    \caption{\textbf{Deriving the LCA thresholding function.} Starting from a pictorial view of the $l_{1}$ cost function, one can derive the self inhibition term from equation \eqref{eq:ch2_u_func_a} and invert it to see the thresholding function described in equation \eqref{eq:ch2_lca_threshold_func}. The value of $\lambda$ dictates the range of allowable sub-threshold membrane potentials, $u_{k}$, before the neuron becomes active.}
    \label{fig:ch2_lca_thresh}
\end{figure}

In addition to finding a sparse code, we are also interested in learning a set of dictionary elements, $\Phi$. This can be done by performing gradient descent on equation \eqref{eq:ch2_sparse_energy} using the coefficient values obtained with the LCA. This yields

\begin{equation}\label{eq:ch2_phi_update}
    \Delta \phi_{k} = \eta (s - \hat{s}) a_{k},
\end{equation}

\noindent where $\eta$ is the learning rate and $\hat{s}$ is the image reconstruction. To recap, for a given image sample we first find our image code, $a$, for a fixed dictionary, $\Phi$, and then using that code to update the dictionary with equation \eqref{eq:ch2_phi_update}.

% TODO diagrams with math comparing feed-forward network to LCA network?
%https://docs.google.com/presentation/d/1Vzk_fVGO-ERQDjKWRvzuHpV6YGwUriD6NyJNpf-WbVE/edit#slide=id.g4f9127c786_0_76

\subsection{The Convolutional LCA}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/lca_conv_benefits.png}
    \caption{\textbf{The convolutional LCA has unique coding properties.} The convolutional LCA (overlap, green lines) has better encoding properties than the patch-based LCA (no overlap, blue lines) in terms of stability and energy. We hypothesize that this is an effect of global competition across the image that is afforded by the convolutional architecture. Note that the patch-based LCA is still sharing weights between patches; it is acting as if each patch was encoded independently with the same model. For video input, we recorded the percent change in coefficients from one frame to the next as a function of the number of inference steps performed on each frame. The left plot shows that convolutional variant is more stable earlier in inference, and approximately equally stable after both variants have converged. Additionally, the convolutional LCA settles to a lower mean squared error between the input and reconstruction (middle plot), and is more sparse (right plot) than the patch-based model. This figure was reproduced from unpublished work done with the Kenyon lab \parencite{paiton2013deep}.}
    \label{fig:ch2_lca_conv_benefit}   
\end{figure}

In this thesis, unless otherwise noted, it should be assumed that we are using the fully-connected variant of the LCA, in which each neuron is connected to every pixel in the input. However, the LCA can also be easily extended to be convolutional to scale it up to larger images. There are benefits to using convolutional over fully-connected LCA: it is more amenable popular GPU computing architectures, it converges to lower minima, and it is more stable when encoding video. However, some drawbacks are that it no longer has a straight forward analog hardware implementation and it diverges from the type of connectivity we see in the brain. A possible better alternative to convolution would be to use local (i.e. tiled) connectivity, as was done in \parencite{le2011building} and \parencite{ngiam2010tiled}. At the time of writing we are unaware of any attempt to use this type of connectivity with the LCA.

We will occasionally use the convolutional LCA to scale up experiments to larger inputs. For a neural-network oriented introduction to convolution, see \parencite{goodfellow2016deep}. The convolution will be parameterized by the kernel size, number of kernels, and stride. If the stride is equal to the patch size (i.e. no overlap), then this model implements the regular patch-based LCA that operates on an entire image worth of patches independently, but simultaneously.

For the convolutional LCA, we write our energy function in terms of a ``deconvolution'', or transposed convolution \parencite{zeiler2010deconvolutional}:

\begin{equation}
    E = \frac{1}{2} || s - a \circledast \Phi ||^{2}_{2} + \lambda ||a||_{1},
\end{equation}

\noindent where $\circledast$ is the transposed convolution operator. We also rewrite the membrane update equation to be in terms of a residual error convolved with the weight vector:

\begin{equation}\label{eq:ch2_conv_lca_dynamics}
   \tau \dot{u_{k}} + u_{k} = e \ast \Phi_{k} + a_{k},
\end{equation}

\noindent where $\ast$ is convolution and $e = s - \hat{s}$ is the residual error. In this variant, the driving force to the neurons is the error, instead of the $b_{k}$ term in equation \eqref{eq:ch2_lca_deda_simple}. This means that every neuron that is competing for the image reconstruction will contribute to driving neuron $k$, even if it is in a different convolutional position. The effects of this global competition across the image appear to give the model an advantage in terms of finding an energy minima and stability, which we demonstrate in figure \ref{fig:ch2_lca_conv_benefit}.


\subsubsection{A note on convolutional overcompleteness}
In patch-based sparse coding, we compute the network overcompleteness by dividing the number of neurons by the number of pixels. However, with convolutional sparse coding there is a caveat that the neighboring inputs have shared weights. Therefore, directly expressing the overcompleteness as number of outputs / number of inputs misrepresents the number of unique weights in the model. Even more nuanced is that the LCA typically learns weights that tile all possible positions in the image patch, which is not as necessary with a convolutional model since the convolution operation translates weights across the image. As a result, all weights learned with convolution tend to be centered in their patch. With all of this in mind, we will denote convolutional overcompleteness as the number of kernels divided by the product of the horizontal (x) and vertical (y) strides: $o = \tfrac{f}{s_{x}*s_{y}}$. Therefore, within reasonable parameter ranges, you can effectively increase the overcompleteness by increasing the number of kernels or by decreasing the stride, which is important when considering computational constraints \parencite{schultz2014replicating}. From this we can also conclude that overcompleteness is not dependent on the patch size, which has been exploited for learning disparity selective neurons that require large receptive fields \parencite{lundquist2016sparse}.


\subsection{Postdictions from the LCA}
% TODO: this paragraph references sec:ch2_alternative_image_coding_models after referencing linear/nonlinear neuron models. I don't currently include a discussion on those models in that section.
%TODO: note haider et al work and adesnik work at the end of this paragraph
In their landmark paper, Olshausen and Field \citeyearpar{olshausen1996emergence} show that the sparse coding model learns oriented, band-pass receptive fields.
These are well matched to fits of the linear approximations of receptive fields of mammalian V1 simple cells obtained via spike-triggered averaging \parencite{hateren1998independent}.
They argue that this supports the hypothesis that the V1 neural population encodes visual stimulus using an efficient wavelet-like dictionary.
This result has been replicated a number of times with various types of sparse coding models \parencite{zylberberg2011sparse, zylberberg2013sparse, rehn2007network}.
As a followup to this study, \parencite{zhu2013visual} show that the LCA network also exhibits a variety of extra-classical (i.e. non-linear) receptive field effects, including end-stopping, cross-orientation inhibition, and surround suppression.
This is an important finding, as it shows that a single LCA network trained on natural scenes matches many linear and non-linear empirical effects found with biological neurons.
Most studies of these non-linear effects employ unique models that are extended from the classic linear/non-linear neuron model for each effect found, while Zhu and Rozell are able to demonstrate many effects with a single model.
In chapter 4 we will argue that these effects result from the explaining away process during sparse inference. %TODO: fix chapter refs
In \parencite{vinje2000sparse}, the authors find that stimulating the nCRF (non-classical receptive field) with naturalistic stimuli causes pairs of V1 neurons to become more decorrelated in their response.
They observed ``dramatically increased sparseness'' when stimulating nCRFs, and suggest that a consequence of this ``is increased independence of the responses across cells''.

As we will expand on in chapter 4, the LCA makes explicit predictions about the existence (and necessity) of population non-linearities among V1 neurons, which facilitate unique responses to natural image stimuli. %TODO: Fix chapter refs
The model gives important attribution for inhibitory lateral connectivity in V1 \parencite{zhu2015modeling}.
In addition to the obvious energy saving motivation for sparse activity, it has been argued that limiting the number of active neurons can result in a more explicit code that is easier to read out by downstream neurons \parencite{olshausen2003principles}.
We will explore this in more detail in later chapters.


\section{Properties of the LCA trained on natural images}\label{sec:ch2_lca_properties}
\subsection{Features learned}
% TODO: Retrain using a different data sample to see if spatial freqs stay anisotropic
% TODO: Are verticle / horizontal RFs also more frequent in v1?
It has been shown previously that sparse coding will learn a dictionary of features that tile spatial position, frequency, and orientation \parencite{olshausen1996emergence, olshausen1997sparse}. We reproduce this finding while also varying the overcompleteness of the model in figure \ref{fig:ch2_lca_overcompleteness_tiling}. We find that as overcompleteness increases, the density of the tiling also increases. Additionally, the model learns higher spatial frequency features and a more even distribution of orientations when more overcomplete. It is also interesting that the models tend to learn significantly more features that are aligned with the vertical and horizontal axis of the input images, which is well matched to image statistics \parencite{switkes1978spatial, torralba2003statistics}. The spatial frequencies also have anisotropy across the vertical axis, which is maintained as you increase overcompleteness. We suspect this is an artifact of the dataset tested, as all three models were trained from the same image patch samples, although we did not test this hypothesis. We also show in figure \ref{fig:ch2_lca_angle_histograms} that when we increase overcompleteness, the neurons have more similar receptive fields as indicated by a smaller angle between weight vectors.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/lca_fits_loc_freq_orien.png}
    \caption{\textbf{Properties of features learned with LCA.} The LCA learns a diverse set of features that tile important signal characteristics when trained on patches from natural scenes. From top to bottom, the rows represent the LCA with 2, 3, and 4 times more neurons than pixels, respectively. From left to right, the columns are the spatial positions, spatial frequencies, and orientations of each function.}
    \label{fig:ch2_lca_overcompleteness_tiling}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/lca_angle_hists.png}
    \caption{\textbf{The LCA neuron weights are closer as overcompleteness increases.} The top row are histograms of the angles between all pairs of basis functions. The bottom row are histograms of the angles to the nearest neighbor for each neuron. From left to right, we increase the network's overcompleteness such that there are 2, 3, and 4 times more neurons than pixels, respectively. The histograms shift left as we increase overcompleteness, indicating that the weight vectors are closer in angle. This will increase competition among neurons because the Gramian matrix term in equation \eqref{eq:ch2_u_dot_full} will have higher values.}
    \label{fig:ch2_lca_angle_histograms}
\end{figure}


\subsection{Inferring sparse codes of natural images with the LCA}
Inference is a critical component of the sparse coding model.
It gives the model the ability to ``explain away'' causes of the input signals \parencite{olshausen1997sparse} and gives neurons a higher degree of selectivity (see chapter 4). %TODO: Fix chapter refs
Figure \ref{fig:ch2_lca_inference_loss} shows the values of the loss function in equation \eqref{eq:ch2_sparse_energy} through inference, demonstrating that the total loss is reduced.
Notice that the sparse loss starts at zero, indicating that none of the neurons are contributing to the reconstruction.
At the beginning of inference, all of the neurons with weights that have a non-zero inner product with the input will increase their membrane potential without producing any output and without receiving any inhibition from other neurons.
It is only after a neuron passes threshold that it starts producing outputs and inhibiting its neighbors.
These sub-threshold dynamics can result in a better solution than the standard ISTA framework, which turns on all units that have a non-zero inner product with the input at the first inference step \parencite{rozell2008sparse}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/lca_inference_loss.png}
    \caption{\textbf{LCA inference minimizes the sparse coding energy function.} The black lines represent the mean loss when encoding 50 natural scenes and the blue background represents the standard error of the mean.}
    \label{fig:ch2_lca_inference_loss}
\end{figure}

Figure \ref{fig:ch2_lca_inference_traces} gives us an idea of the inference dynamics for a subset of neurons in the LCA network. Each line in the trace plots represents a different term from equation \eqref{eq:ch2_lca_deda_simple}. Nearly all of the traces follow nonlinear trajectories and have interesting sub-threshold dynamics. Figure \ref{fig:ch2_lca_inference_weights} shows the corresponding weights for each of the neuron traces in figure \ref{fig:ch2_lca_inference_traces}.


\subsection{The hard thresholded LCA has many energy minima}\label{sec:ch2_hard_lca}
It is possible to define a variety of valid threshold functions for converging LCA dynamics. The two primary functions considered in \parencite{rozell2008sparse} are termed ``soft'' and ``hard'' thresholds, which approximate solutions to the $l_{1}$ and $l_{0}$ cost functions, respectively. Much of the work herein is focused on the soft threshold function (equation  \ref{eq:ch2_lca_threshold_func}), primarily for model simplicity. However, the hard thresholded variant is intriguing as it results in a higher degree of non-linearity and solves a harder (non-convex) optimization problem with many equally valid local minima. The rectified hard thresholded LCA model behaves exactly as is described in section \ref{sec:ch2_lca}, except that the thresholding function has been changed to:

\begin{equation}\label{eq:ch2_lca_hard_threshold_func}
    T_{\lambda}(u_{k}(t)) = \left\{
    \begin{aligned}
        0,\;\; &u_{k}(t)\; \leq\; \lambda \\
        u_{k}(t),\;\; &u_{k}(t)\; >\; \lambda
    \end{aligned}
    \right.
\end{equation}

As is described in \parencite{rozell2008sparse}, this variant approximates the $l_0$ cost function for $C(\cdot)$ in equation \eqref{eq:ch2_sparse_energy}. The hard thresholded LCA performs similarly to the soft thresholded variant, except that there is no longer a convex energy landscape with a single global minimum. After some number of inference steps, the LCA settles to a stable minima, but in \parencite{shainin2016sampling} we show that the minima reached by a convolutional, rectified, hard thresholded LCA is not a global minima. It is not clear how the amount of information differs between these minima. However, it has been proposed that the brain could represent likely events in a probabilistic framework \parencite{lee2003hierarchical} and that sampling could be a mechanism for decoding signals in higher cortical areas \parencite{hoyer2003interpreting}.

% TODO: verify the distance metric we used - neurons that turned on / off or neurons that changed activity?
To better understand the energy landscape, we sampled a number of approximately equivalent minima (in terms of energy) for sparse codes produced from weights trained on the CIFAR10 dataset \parencite{krizhevsky2009learning}. We performed this sampling by first allowing the network to settle to a minima for a given image, then perturbing the neuron activations with Gaussian noise, then allowing it to settle again, and repeating for some fixed number of perturbations. We measured the distances between fixed points by counting the number of neurons that changed from active to inactive, which is equivalent to the Hamming distance between binarized (changed threshold or did not) vectors. Figure \ref{fig:ch2_lca_fixed_point_distances} shows that the Hamming distance from the original fixed point to the perturbed fixed point grows as we continue to perturb the network until it reaches a plateau. The figure also shows that the network \textit{always} settles on a different minima.

%TODO: add label to right plot (sample number) and to colorbar (hamming distance)?
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lca_fixed_point_distances.png}
    \caption{\textbf{Fixed points of the hard thresholded LCA.} The hard thresholded LCA has a number of valid fixed points in terms of the energy defined in equation \eqref{eq:ch2_sparse_energy}. If you allow the model to settle to a minima, and then perturb the latent code with Gaussian noise, it settles to a different minima. On the left we plot the Hamming distance between codes from the original fixed point to the perturbed fixed point after many perturbations. The Hamming distance is measured by assessing how many neurons crossed threshold from active to inactive or vice versa. On the right we plot the Hamming distances between all pairs of fixed points for all perturbations. All of the off-diagonal values are above 0, which indicates that the model always settled to a unique minima after perturbation. This figure was reproduced from unpublished work with permission from \parencite{shainin2016sampling}.}
    \label{fig:ch2_lca_fixed_point_distances}
\end{figure}

In figure \ref{fig:ch2_lca_norms_and_acc}, we show that the $l_{0}$ sparsity of the activations improves as we continue to perturb the input while the reconstruction error remains the same, suggesting that it is finding better minima. To understand the information content of these minima, we trained two layer classifiers on the sparse codes produced after convergence. The first experiment was to average the fixed point vectors together. We show that the classification accuracy increases as we add more fixed points to the average, which implies that there is different information content per fixed point. The second experiment was to search the independent fixed points for whichever produced the highest confidence (in terms of minimum entropy of the classifier's output distribution) and use this single fixed point as input to the network. The right panel in figure \ref{fig:ch2_lca_norms_and_acc} shows that the classifier accuracy improves as we increase the number of candidates in the search. Additionally, we found that the number of perturbations to achieve a fixed point with highest confidence was highly variable (not shown).

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/lca_norms_acc.png}
    \caption{\textbf{Fixed points have different sparsity and contribute unique information for a classification task.} The plot on the left indicates that new fixed points after perturbing the hard thresholded LCA network tend to have fewer active elements. The blue and green lines show that norms increase when we average together the codes from each perturbation, which is to be expected. The cyan and red curves show that as we continue to perturb the activations, the network settles on a minima with approximately equal $l_{1}$ norm and an improved $l_{0}$ ``norm'', or active neuron count. For the middle plot we constructed an averaged vector from each fixed point and used that as the input to a classifier network. The classifier accuracy improves as the number of vectors in the average grows, indicating that there is additional information in the alternate fixed points. As a followup experiment, instead of averaging the fixed points, we searched them to determine which one produces a minimum entropy output from the classifier. This fixed point was then used to determine the class label. The right most figure shows that as you increase the number of samples in the search, the classifier accuracy improves. This figure was reproduced from unpublished work with permission from \parencite{shainin2016sampling}.}
    \label{fig:ch2_lca_norms_and_acc}
\end{figure}


\section{Alternative image coding models}\label{sec:ch2_alternative_image_coding_models}

% TODO:
%\subsection{The linear / non-linear neuron model}
%Simoncelli, etc. Adding components.
% mathematical comparison from L/NL model to RAE & SAE
%
%\subsection{Autoencoders}
%Autoencoders as a NN implementation of L/NL neurons.


\subsection{Predictive coding}
Here we will draw comparisons between the LCA and the predictive coding model \parencite{rao1999predictive}. The LCA encoder is a single layer predictive coding encoder with a linear synthesis function. This is made intuitive by comparing the LCA against the predictive coding model.

The objective of this investigation is to explore the encoding processes for sparse coding and predictive coding assuming that we are given a trained dictionary. In our sparse coding energy function (equation \eqref{eq:ch2_sparse_energy}), we use $\lambda$ as a trade-off penalty between the reconstruction quality and the sparsity constraint. In practice we tune it to maximize network sparsity for a minimum accepted reconstruction quality. In Olshausen and Field \citeyearpar{olshausen1997sparse}, their choice of cost function, $C(.)$, specified a Cauchy distribution for the prior on $a$:

\begin{equation}\label{eq:ch2_cauchy_cost}
  C(a_{i}) = \log(1+a_{i}^{2})
\end{equation}

For a learned dictionary, to encode an input image we compute a coefficient vector, $a$. Our coefficients (neuron activations) adapt to minimize the energy function described in equation \eqref{eq:ch2_sparse_energy}. The standard way to do this is to perform direct gradient descent on the energy function, following equation \eqref{eq:ch2_lca_deda_extended}, which we will rewrite here with a small algebraic change:

\begin{equation}\label{eq:ch2_sc_deda_rewrite}
    - \frac{\partial E(t)}{\partial a_{k}(t)}
    =
        \sum\limits_{i}^{N} \Phi_{i,k} \left(S_{i} - \sum\limits_{j}^{M}a_{j}(t) \Phi_{i,j}\right) -
        \lambda \sum\limits_{j}^{M}\frac{\partial C(a_{j}(t))}{\partial a_{k}(t)}
\end{equation}

A block diagram illustration of this encoding process is given in figure \ref{fig:ch2_lca_pc_comp}.

Much like the sparse coding model, the predictive coding model is a neural network that aims to implement efficient coding for natural visual stimuli \parencite{rao1997dynamic, rao1999predictive}. Although the predictive coding model is described in a more general fashion, the actual implementation used for experiments ends up being strikingly similar to the sparse coding model. Assuming a learned dictionary, the energy equation for the predictive coding model is as follows:

\begin{equation}\label{eq:ch2_pc_energy_func}
        E =
        \tfrac{1}{\sigma_{S}^{2}} \|S - \hat{S} \|_{2}^{2} +
        \tfrac{1}{\sigma_{td}^{2}} \|a - a^{td}\|_{2}^{2} +
        \lambda \sum\limits_{i}^{M}C(a_{i}),
\end{equation}

where $a^{td} = f\left(\Phi^{1T}a^{1}\right)$ represents the top-down feedback and $\sigma_{td}^2$ is the expected Gaussian variance of the $a^{0}$ estimate. As they describe it, this is a combination of the ``sum of squared prediction errors for level 1 and level 2, each term being weighted by the respective inverse variances.'' The energy function also includes a cost on the activations, which for this example will still be derived from the Cauchy prior. A key difference here between this model and the sparse coding model is how the image approximation, $\hat{S}$ is computed. Rao and Ballard describe a non-linear synthesis function, $f(.)$, which is used in the reconstruction:

\begin{equation}\label{eq:ch2_pc_synthesis}
 \hat{S} = \sum\limits_{i}^{M}f(\phi_{i}a_{i})
\end{equation}

If $f(.)$ is the identity function (i.e. $f(a)=a$), then we get the sparse coding reconstruction energy term. Rao and Ballard describe this as a ``lateral inhibition'' model, although the form of synthesis does not actually differentiate between a feedback or lateral inhibition model. In their study, they experiment with both an identity synthesis function, $f(a)=a$, and a hyperbolic tangent synthesis function, $f(a)=tanh(a)$.

Again we can take the derivative of equation \eqref{eq:ch2_pc_energy_func} to get our update rule. In \parencite{rao1999predictive} they include a time constant for the iterative update procedure, which we will leave out for clarity.

\begin{equation}\label{eq:ch2_pc_deda}
    - \frac{1}{2}\frac{\partial E}{\partial a_{k}}
    =
        \frac{1}{\sigma_{S}^{2}}\sum\limits_{i}^{N} \Phi_{i,k} \left(S_{i} - \sum\limits_{j}^{M}a_{j} \Phi_{i,j}\right) -
        \frac{1}{\sigma_{td}^{2}}\sum\limits_{i}^{M}(a_{i}-a_{i}^{td}) -
        \lambda \sum\limits_{j}^{M}\frac{\partial C(a_{j})}{\partial a_{k}}
\end{equation}

Comparing equations \eqref{eq:ch2_pc_deda} and \eqref{eq:ch2_sc_deda_rewrite}, one can conclude that the two differences between the predictive coding model and sparse coding model are the (potential) use of a non-linear synthesis function and a second layer of processing. This is illustrated pictorially in figure \ref{fig:ch2_lca_pc_comp}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{./figures/lca_pc_model_comparisons.png}
    \caption{\textbf{Comparing the LCA to Predictive Coding.} Sparse approximation can be performed via direct gradient descent on the energy function defined in equation \eqref{eq:ch2_sparse_energy}, which we depict as a dynamic process. Adding a non-linearity to the LCA synthesis function results in a network that is equivalent to a single layer of the predictive coding model. The full model combines two such layers \parencite{rao1999predictive}}
    \label{fig:ch2_lca_pc_comp}
\end{figure}


% TODO:
%\subsection{Iterative Subtraction as a Form of Divisive Normalization} \label{divnorm}
%In their nature publication \cite{rao1999predictive}, Rao \& Ballard suggest that repetitive subtraction of neighboring neuronal activities may produce a net effect similar to divisive normalization. Eero's group has look extensively into divisive normalization - this section is intended to attempt to draw a link between the two.


\subsection{ICA}
Sparse coding and ICA are related algorithms that employ similar linear generative models, but differ critically in their encoding processes. The image code produced by ICA is computed linearly, whereas sparse coding computes the code with a nonlinear inference process. In section \ref{sec:ch4_selectivity_efficiency}, we show that this nonlinear encoding process confers a considerable advantage in coding efficiency and orientation selectivity.

ICA assumes the generative model

\begin{equation}\label{eq:ch2_ica_generative_model}
    s = \Phi a,
\end{equation}

\noindent where $\mathbf{s}$ is an image, $\mathbf{\Phi}$ is a matrix of filters, and $\mathbf{a}$ is a vector of activations that represents the neural code \parencite{bell1997independent}. The goal of ICA is to learn a set of statistically independent filters such that the input data can be reconstructed with minimal error. In ICA, the filter matrix is square and full rank and thus invertible. This allows the ICA activations for a given input to be computed directly as:

\begin{equation}
    \hat{a} = \Phi^{-1}s
\end{equation}

During inference, activations are computed with a single, linear, feed-forward operation. The filter matrix is learned via a non-linear, iterative optimization process to accurately reconstruct the input while maximizing the statistical independence of the filters as measured by the joint entropy of the activations. This process results in filters that are optimized for the higher-order statistical structure in natural scenes.

Sparse coding, however, has a highly non-linear encoding process. The overall optimization procedure involves a fast inner loop in which the coefficients are computed for each data vector and a slower outer loop in which the basis functions are adapted to the statistics of the entire dataset. In the inner loop, coefficients are computed considering the prior on their expected distribution. In ICA, however, the prior plays no role in determining the coefficients, but it does still play an important role in the learning the basis functions.

The ICA learning algorithm is simpler and faster than the sparse coding algorithm because the encoding can be computed from the data in a single feedforward pass. The independent component analysis algorithm of Bell and Sejnowski \citeyearpar{bell1997independent} is formally equivalent to maximum likelihood estimation in the case of no noise and a square system (the dimensionality of the output is equal to the dimensionality of the input). It is easy to generalize this to the case when the number of outputs is less than the number of inputs, but harder the other way around. When the number of outputs is greater than the effective dimensionality of the input the extra dimensions of the output will simply drop out \parencite{livezey2016degeneracy, le2011ica}. While this is not as important for blind separation problems where the number of independent sources is less than or equal to the number of mixed signals, it will become a concern in the representation of images, where overcompleteness is a desirable property \parencite{simoncelli1991shiftable}. The main difference between the Olshausen and Field \citeyearpar{olshausen1996emergence} sparse coding model and the ICA algorithm of Bell and Sejnowski \citeyearpar{bell1997independent} is in the simplifying assumptions they make in order to deal with the intractable integration problem posed by finding the set of basis functions that maximize the likelihood of the inputs given the model defined by those basis functions. Olshausen and Fields algorithm assumes low-noise and a peaky, unimodal distribution on the joint probability between the image and the sparse code given the model in order to justify evaluating it at the maximum. Bell and Sejnowski limit the dimensionality of the code to equal the dimensionality of the input, assume no noise, and enforce an orthogonal set of basis functions so that the integral becomes tractable and can be analytically computed.

% TODO: BF learned with ICA vs complete LCA ; the LCA is still non-linear encoding when complete.

\section{Conclusion}
In this chapter we derived the LCA from a probabilistic perspective. We then showed the features learned by the LCA when trained on natural scenes and highlighted some interesting properties of the inference process. Finally, we compared the LCA to alternative image coding models. In the next chapter we will extend the LCA model to a semi-supervised learning framework and a hierarchical framework. In the following chapter we demonstrate how a geometric analysis technique can be used to explain the LCA's high degree of robustness to noise and selectivity when compared to alternative models.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lca_inference_weights.png}
    \caption{\textbf{Weights learned when the LCA is trained on natural scenes.} Like other sparse coding variants, dictionary learning with LCA inference produces weights that have a high degree of correspondence to those recorded from mammalian V1 \parencite{zylberberg2011sparse, rehn2007network, olshausen1997sparse}. The weights in this plot correspond to the same neurons as were used to generate the traces in figure \ref{fig:ch2_lca_inference_traces}.}
    \label{fig:ch2_lca_inference_weights}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/lca_inference_traces.png}
    \caption{\textbf{Individual trace dynamics for LCA neurons.} Shown are input and output traces for a subset out of 768 LCA neurons during inference. The input image is in the bottom left. Each trace plot has a corresponding weight, shown in figure \ref{fig:ch2_lca_inference_weights}.  The lines in the trace plot represent terms from equation \eqref{eq:ch2_lca_deda_simple}, as indicated in the legend. The grey bars on the left of each trace plot indicate relative scale of the vertical axis. A magenta boundary indicates that the neuron was active at the end of the inference period, and a black boundary indicates otherwise. A dotted boundary indicates that it became active or inactive in the last 20 percent of the inference process.}
    \label{fig:ch2_lca_inference_traces}
\end{figure}