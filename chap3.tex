\chapter{Iso-Response Contours Provide a Geometric Interpretation of Neurons}

\section{Methods for measuring response curvature}
From neuroscience

\section{Pointwise nonlienarities}
straight contours

\section{Population nonlienarities}
bent contours

\section{Iso-response contours}
relu, deep relu, sigmoid, lca, divisive normalization

\section{Explaining increased orientation selectivity}
A longstanding hypothesis in visual neuroscience is that sensory neurons are adapted to natural image statistics to produce an efficient code. Independent Component Analysis (ICA) has been proposed as a normative model for simple cells in V1 based on its ability to reduce higher-order redundancy in natural images. When applied to natural images, the filters that emerge resemble the localized, oriented, and band-pass receptive fields of V1 neurons. However, quantitative analyses of the coding efficiency of ICA show that the neural code it produces fails to provide any appreciable gain in redundancy reduction beyond second-order methods such as PCA. This result appears to challenge the higher-order redundancy reduction account of V1 function. We explain these findings by distinguishing oriented filters from orientation \textit{selectivity} and show that ICA's linear encoding scheme fails to implement genuine orientation selectivity, which limits its capacity to learn an efficient code. We show that sparse coding, a related model with a nonlinear encoding scheme that produces orientation selective neurons, is able to achieve a more efficient code than both ICA and PCA when evaluated in the rate-distortion framework, thus providing renewed support for the efficient coding account of V1 receptive field properties.

A long-standing hypothesis in sensory neuroscience proposes that a primary function of early sensory processing is to form an efficient, redundancy-reduced code of the input that maximizes the brain's limited computational and biological resources while making explicit the statistical structure of the input \cite{barlow2001redundancy}. This hypothesis predicts that the response properties of sensory neurons should be adapted to the statistical structure of their input.

In support of this hypothesis, a number of the response properties of visual neurons have been reproduced by optimizing redundancy-reducing linear transformations on natural images \cite{atick1990towards}. For example, a symmetric decorrelation transformation of natural images yields center-surround receptive fields \cite{atick1990towards}, and Principal Component Analysis (PCA) applied to color images yields the luminance, red-green, and blue-yellow channel observed in the opponent color coding of retinal ganglion cells \cite{ruderman1998statistics, buchsbaum1983trichromacy}. When higher-order correlations are additionally reduced, localized and oriented band-pass filters that resemble the orientation-selective receptive fields in V1 emerge \cite{bell1997independent, olshausen1999probabilistic}. It has thus been proposed that the oriented filters in V1 function to remove higher-order correlations.

Orientation selectivity is a striking feature of the response properties of simple cells in V1. Since the discovery of orientation selectivity in Hubel and Wiesel's Nobel Prize-winning work, the mechanism for the computation has remained unclear. A common point of confusion in the field has been the assumption that a neuron with an locally oriented receptive field will exhibit orientation selectivity. Here, we will argue that orientation selectivity requires a non-linear encoding process in addition to an oriented receptive field.

Independent Component Analysis (ICA) is one of the most widely used image coding algorithms and has been proposed as a model for simple cells in V1 \cite{bell1997independent}. The ICA algorithm explicitly optimizes for higher-order redundancy reduction, aiming to to reconstruct the input image as a linear superposition of a set of basis functions while minimizing the mutual information between those bases.

\citeit{eichhorn2009natural} compare the  coding efficiency of ICA and PCA to obtain the surprising result that ICA performs no better than PCA on a rate-distortion trade-off metric. ICA is trained with the objective of minimizing the joint entropy of the activations and learns oriented filters that suggest it has succeeded in modeling higher-order pixel correlations, while PCA is a second-order method that does not learn oriented filters. \citeit{eichhorn2009natural} argue that if ICA had succeeded at capturing higher-order statistics, it should show an advantage in the rate-distortion trade-off.

We present an alternate explanation of these findings by distinguishing \textit{orientation selectivity} from \textit{oriented filters}. Neurons achieve orientation selectivity via a fundamentally nonlinear process, as exhibited by nonclassical receptive field (nCRF) effects such as cross-orientation suppression \cite{golden2016conjectures, zhu2013visual}. We argue that although the ICA optimization algorithm is able to learn oriented filters, ICA's linear encoding process limits its capacity to perform genuine orientation selectivity, which in turn limits its capacity to produce an efficient code. \citeit{zhu2013visual} demonstrate that sparse coding is able to provide a parsimonious explanation of both classical and nonclassical receptive field properties using a neural network implementing sparse coding \cite{zhu2013visual}. Here, we replicate the rate-distortion analyses from \citeit{eichhorn2009natural} using the sparse coding network described by \citeit{zhu2013visual} to show that a nonlinear encoding process produces more efficient codes to linear encoders. To assess the degree of orientation selectivity for ICA neurons, we replicate the cross-orientation suppression experiment performed by \citeit{zhu2013visual} with both sparse coding and ICA networks.

\subsection{Rate-distortion analyses}
The Shannon standard for evaluating the efficiency of lossy continuous codes is the rate-distortion framework \cite{cover2012elements}. \citeit{eichhorn2009natural} compare the  coding efficiency of ICA and PCA and find that PCA performs \textit{better} than ICA in terms of the rate-distortion trade-off. This result is surprising in that ICA is explicitly trained with the goal of minimizing the joint entropy of the activations and learns oriented filters that would suggest that it achieved the goal of modeling higher-order correlations, while PCA is a second-order method that does not learn oriented filters.

We resolve this apparent paradox by distinguishing \textit{orientation selectivity} and \textit{oriented filters}. Neurons achieve orientation selectivity via a fundamentally non-linear process, as exhibited by non-classical receptive field (nCRF) effects such as contrast invariant tuning and cross-orientation suppression \cite{ferster2000natural,  zhu2013visual}. We argue that although the ICA optimization algorithm is able to learn oriented filters, ICA's linear encoding process limits its capacity to perform genuine orientation selectivity, which in turn limits its capacity to produce an efficient code. Sparse coding is unique in its ability to provide a parsimonious explanation of both classical and non-classical receptive field properties \cite{zhu2013visual, golden2016conjectures}. Although nCRF effects are typically modeled individually, \citeit{zhu2013visual} show that a wide variety of these effects are emergent properties of a neural network implementing sparse coding. * something about LCA * These findings suggests the primacy of efficient coding in V1. This explanation only holds, however, if sparse coding can be quantitatively shown to achieve a gain in coding efficiency beyond second-order methods. Here, we replicate the rate-distortion analyses from \citeit{eichhorn2009natural} and show that sparse coding's non-linear encoding process enables codes that are lower entropy than those learned by ICA or PCA while being more perceptually robust to increasingly coarse quantization.

\subsection{Methods and Results}
We train sparse coding \cite{rozell2008sparse}, ICA \cite{bell1997independent}, and PCA on 1 million 16 x 16 pixel grayscale image patches extracted from images in the van Hateren dataset of natural scenes, which have been transformed to log intensity and standardized to zero mean and unit variance \cite{vanHateren1998independent}. Using the learned filter matrices, we compute model activations for a test set of 100,000 patches and uniformly quantize these activations with varying degrees of granularity. For each level of granularity, we compute a reconstruction of the test input using the quantized activations and compute the mean squared error. Figure \ref{fig:rd_curve} plots the rate (mean marginal discrete entropy of the activations) against the distortion (mean squared error). 

%\begin{figure}[ht]
%\vskip 0.1in
% \centering \includegraphics[width=\linewidth]{figures/rd_curves.png}
% \caption{Discrete entropy vs. reconstruction error for sparse coding, PCA, and ICA.}
%\vskip -0.2in
%\label{fig:rd_curve}
%\end{figure}

Results for sparse coding models trained with different values of $\lambda$ are shown, where a larger $\lambda$ indicates higher sparsity. We replicate the findings from \citeit{eichhorn2009natural} that (orthogonal) PCA  performs slightly better than ICA in the rate-distortion trade-off. Sparse coding shows an advantage over both ICA and PCA. Additionally, we find that representations that are more sparse are capable of achieving increasingly lower rates.
Figure \ref{fig:recons} shows an example reconstruction in the highly lossy (low entropy) regime. For a mean marginal entropy of $H\approx 0.4$, sparse coding shows an advantage in perceptual quality, as well as a quantitative advantage in terms of mean squared error.

%\begin{figure}[ht]
%\vskip 0.1in
% \centering
% \includegraphics[width=\linewidth]{figures/baboon_4_square.png}
% \caption{Lossy reconstructions for quantized activations with mean marginal entropy $H\approx 0.4$}
%\vskip -0.2in
%\label{fig:recons}
%\end{figure}

\subsection{Discussion}
Our results suggest the importance of nonlinear encoding for learning efficient codes of natural images and demonstrate that orientation \textit{selective} neurons are capable of reducing higher-order redundancy. We show that although the ICA algorithm is able to learn oriented filters, ICA's linear encoding process limits its capacity to perform genuine orientation selectivity, which in turn limits its capacity to produce an efficient code. 

Although sparse coding produces a more efficient representation of natural image and neurons that have a higher degree of orientation selectivity than ICA, the model is nonetheless fundamentally limited in its capacity to fully characterize the statistics of natural scenes because it assumes a linear generative model and the light that forms images is combined in a non-linear fashion, such as by occlusion. In future work we plan to extend our analysis to hierarchical models of natural scenes that may achieve greater gains in coding efficiency.

The efficient coding hypothesis was initially posed in terms of redundancy reduction \cite{barlow1961possible}, under the hypothesis that the brain may seek an efficient code of the input in order to minimize the number of neurons required to represent the signal. Anatomical evidence tells us, however, that the V1 expands the image representation coming from LGN by having many more outputs than inputs \cite{olshausen2003principles} Thus, redundancies are actually \textit{created} in the perceptual process. The goal of cortical processing, then, cannot be said to be redundancy reduction and simple compression.

As an alternative, several researchers have argued that the goal of perception cannot be discussed in isolation from action; an organism forms perceptual representations for the purpose of directing its behavior towards the achievement of desirable outcomes and away from undesirable ones \cite{barlow2001redundancy, simoncelli2001natural}. From this perspective, the brain aims to extract the statistical structure of the input in order to form a``meaningful'' representation that recovers the environmental causes of the sensory data, which it can use to guide action. Along these lines, the efficient coding hypothesis has been revised to emphasize redundancy \textit{representation} rather than reduction \cite{barlow2001redundancy}. Redundancies in the input signal indicate structure in the environment. An encoding that makes these redundancies explicit encodes the causal and statistical structure of the environment, which the organism can exploit to plan and direct behavior.

Sparse coding performs redundancy representation rather than redundancy reduction. A sparse code is a highly redundant code It has been demonstrated that a typical redundancy reducing code--which would form a distributed representation of the input with a high activity ratio--would actually lead to large errors in estimates of the frequency of a particular input, since many neurons are active in response to both the input of interest as well as other stimuli \cite{gardnermedwin2001limits}. A sparse code, in which the elements of the learned dictionary occur independently in the environment, is a factorial code; the probability of any composite image is simply the product of the probabilities of the components. Any deviations from this rule signal a previously unknown statistical dependency to be learned.

 An efficient code exploits the redundancies in the input signal. The objective of early sensory processing was initially described as redundancy reduction. The redundancy reduction hypothesis was partially motivated by the observation that a significant information bottleneck exists in the first stage of visual processing; most mammals have vastly more photoreceptors than fibers in the optic nerve, which suggests that significant compression must occur in the first stage of processing. However, at moderate to high luminance levels, only a small subset of the photoreceptors are operating within their dynamic ranges; thus the reduction in capacity may be smaller than initially implied. Further, beyond the optic nerve, the number of neurons involved in subsequent layers of processing generally increases, which means that redundancies are actually created in the perceptual process. The goal of early vision, then, cannot be said to be redundancy reduction. From a functional standpoint, several researchers have also argued that the goal of perception is not simple compression; an organism forms perceptual representations in order to direct its behavior towards the achievement of desirable outcomes and away from undesirable ones \cite{barlow2001redundancy, simoncelli2001natural}. From this standpoint, one can argue that brain aims to extract the statistical structure of the input in order to form a \textit{meaningful} representation that recovers the environmental causes of the sensory data, which it can use to guide action.
 
 Along these lines, the efficient coding hypothesis has been revised more recently to emphasize redundancy representation rather than reduction \cite{barlow2001redundancy}. Redundancies in the input signal indicate structure in the environment. An encoding that makes these redundancies explicit encodes the causal and statistical structure of the environment, which the organism can exploit to plan and direct behavior. Barlow further argues that to facilitate the identification of the statistics of the environment, neural responses should form a sparse code of the input. He notes that a typical redundancy-reducing code would be a distributed representation of the input with a high activity ratioâ€“that is, a large percentage of active neurons, each of which is frequently active across different inputs. Such a code will lead to large errors in estimates of the frequency of a particular input, since many neurons are active in response to both the input of interest as well as other stimuli. A sparse code, in which the elements of the learned dictionary occur independently in the environment, would form a factorial code, in which the probability of any composite image is simply the product of the probabilities of the components. Any deviations from this rule would signal a previously unknown statistical dependency to be learned.

\section{Explaining extra-classical receptive field effects}
Golden, explaining others.

\section{Predicting adversarial perturbations}
Adversarial stuff.

\section{Applications to physiological neuroscience}