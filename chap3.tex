\chapter{Hierarchical LCA}

\section{Related hierarchical sparse coding models}
Ultimately, we aim to build a hierarchical general representation of natural scenes. A hierarchical model of natural scenes should produce a general representation of input data that spans all layers. The bottom of the hierarchy contains information about the details of the scene, and as one ascends the hierarchy, one should see a more general, abstract description. Information should move up the hierarchy in the form of inference and down the hierarchy as expectations or priors. Resolving ambiguities at the top should be easier as there are more regularities and there is more context. These should help inform inference in lower layers to resolve more difficult ambiguities about details of the scene.

Broadly speaking, there have been many hierarchical unsupervised learning architectures in the literature. Here we will highlight work that is particularly relevant considering the goals of the previous paragraph. Lee and Mumford \citeyearpar{lee2003hierarchical} propose a model that comes very close to what was described in the previous paragraph. They simplify the model slightly by forcing each layer to only receive input from the layer below & above, as in a Markov chain. Expectations are propagated down to alter priors of lower layers in a dynamic, context-sensitive manner. Feedback connections influence the inference process, causing a layer to converge on an alternate solution that fits the expectations above. Although they use dynamic sampling algorithms to represent Bayesian inference, we believe it is possible to construct a version that is true to the theory and uses sparse coding as a core computational framework. We identify this as an exciting area of future research. Karklin and Lewicki \citeyearpar{karklin2003learning} propose a hierarchical probabilistic model that is an extension of the ICA model. They start with learning a complete orthogonal basis with ICA, and then learn ``variance bases'' that model higher-order structure in images. Their model produces a hierarchical sparse distributed code of natural scenes, although they do not perform inference but instead directly compute activations in a feed-forward architecture.

Shan and Cottrell \citeyearpar{shan2013efficient} 

Cadieu and Olshausen \citeyearpar{cadieu2008learning} propose a two-layer sparse coding network that explicitly disentangles form and motion in the second layer when trained on video input. The first layer is a sparse coding model that is trained with complex valued dictionary elements. They then factorize the first layer output into two sets of second layer inputs that encode the time-derivative of the amplitude and phase components to explicitly separate form and motion, respectively. The second layer units also perform sparse coding on the logarithm of factorized quantities from the first layer. They demonstrate that form selective neurons develop invariance properties from the time-varying signal. A convolutional LCA variant called the Deconvolutional Competitive Algorithm (DCA) is proposed in \parencite{paiton2015deconvolutional} that deconstructs images into a three-layer hierarchical code. The network performs inference in all three layers simultaneously, and all three layers compete to reconstruct the image. By configuring the strides and patch sizes appropriately, the network was designed such that each layer contributes a different degree of detail. The lowest layer reconstructed high spatial frequency, grayscale details while the highest layer reconstructed low spatial frequency and color information.

The sparse manifold transform was proposed by \parencite{chen2018sparse}, which is an extendable hierarchical sparse coding network that models the underlying manifold structure of time-varying natural scenes. 

\subsection{PCA dimensionality reduction to learn second layer units}
Hyvarinen & Hoyer strong PCA work; my project implementing this with LCA


\section{Weakly-Supervised Feature Learning}

\subsection{Introduction}
We evolved a visual sense to allow us to understand the external causes of incident light. The sensor modality is not designed to construct a veridical representation of the world \parencite{gollisch2010eye}. Instead, it is designed to allow us to identify objects of significance and act upon them. Our visual system is intimately connected with the statistics of light as it propagates through our natural world. These statistics have been analyzed extensively by scientists exploring images and videos of natural scenes (see chapter \ref{ch:intro}. The sparse coding model represents an attempt to build on the knowledge gained from studying the statistics of natural scenes to better understand our visual system. In the field of visual unsupervised machine learning, a cornucopia of models have been proposed to learn the statistics of natural scenes without human labels, or ``supervision'' \parencite{baldi2012autoencoders, bengio2012unsupervised, goodfellow2016deep}. It is important to recognize that the unsupervised objective function of most of these models does in fact ask for a veridical representation of the inputs, without any real consideration for other ecological significance for the latent code produced. None the less, when combine with a family of constraints that include minimum entropy, maximum compression, and minimal energy expenditure, autoencoder models can exhibit interesting properties that are also found in biological vision systems. In this chapter, we consider the LCA as an autoencoder. Like an autoencoder, the LCA receives inputs, transforms them into a latent code, and produces reconstructions. We are interested in understanding how useful LCA can be for the machine learning field. One way to assess this is to look at semi-supervised learning. Here the objective is the same as for supervised learning, where we want to associate images with some predetermined category label. However, the catch is that many of the training images do not have ground truth labels assigned to them. A fully supervised model would not be able to use these, and would suffer from limited training examples. Here we show that LCA can be used as an agent to improve semi-supervised learning results. We also demonstrate how an alternate objective, like labeling objects in the world, can be used in the LCA dynamics to modify inference and dictionary learning.

\subsection{Weakly-supervised learning}
%[https://docs.google.com/presentation/d/1Dy_Dy1uSnLC3FEWXczgdKGxSejRUYQmfHnwPtY5LA8Y/edit#slide=id.g12e96bb738_0_271]
%[https://docs.google.com/presentation/d/1CcFmB1AUIEWU_rKtIaiRM79QGMetjYhnjKfv58hDD8Y/edit#slide=id.g19049d0ee0_0_22]
%[https://docs.google.com/document/d/13IzufcIS9M9HTCKsQGPSWmCB7lbm4axBAVbGeEml-ks/edit?usp=sharing]
%[https://docs.google.com/presentation/d/1bW__4dYIlrrbiV55Y1WdKcI_jxYqCSPXVpQjnumrDfo/edit#slide=id.g13eab2f708_0_83 slide 11]

Human labels are extremely expensive and often biased. A learning paradigm that avoids this process is unsupervised learning, but it is not directly applicable to the machine learning task of assigning labels to, or categorizing, data. Weakly-supervised learning is a sub-field that aims to combine the benefits of unsupervised and supervised learning. An ideal model should learn to categorize (e.g. cluster) data without ground-truth labels while still maintaining a faithful representation. As we demonstrate in chapter \ref{ch:iso}, sparse coding produces a code that is both descriptive and faithful to the image content. Here, we wish to modify the sparse coding model to utilize limited label information about an input scene.

\subsubsection{Features learned in unsupervised frameworks match those learned in supervised frameworks}
The Discriminative Recurrent Sparse Auto-Encoder (DrSAE, \cite{rolfe2013discriminative}) behaves similarly to sparse coding and has been shown to be successful at semi-supervised learning. Unlike LCA, the encoder, decoder, and lateral connectivity weights of DrSAE are unconstrained and trained independently. However, when trained on the MNIST dataset \parencite{lecun1998mnist}, the DrSAE learns weights that closely match what we impose for sparse coding. That is, the decoder weights have high correspondence to the transpose of the encoder weights and the lateral connectivity weights have high correspondence with the Gramian of the encoder weights. Additionally, when semi-supervised training is performed, the model learns an emergent hierarchical architecture. Figure \ref{fig:ch3_lenet_lca_drsae_weights} shows that the features learned by the unsupervised LCA and DrSAE models have a high degree of structural similarity to those learned by a supervised model \parencite{lecun1998gradient}.

\begin{figure}\label{fig:ch3_lenet_lca_drsae_weights}
    \centering
    \includegraphics[width=0.25\textwidth]{figures/lenet_lca_drsae_weights.png}
    \caption{\textbf{Unsupervised learning leads to similar features.} The features learned from unsupervised training on MNIST using LCA \parencite{rozell2008sparse} and DrSAE \parencite{rolfe2013discriminative} have similar structure to those learned using the supervised LeNet model \parencite{lecun1998gradient}}
\end{figure}

The DrSAE model was also extended to a semi-supervised learning framework. Given their success, we constructed a similar framework with LCA. The LCA dynamics are derived from a principled energy function, so we were able to extend the framework by adding semi-supervised loss terms to the energy function. Typically in sparse coding the sparsity enforcing term is applied uniformly, penalizing all nodes. Instead of the prior limiting the total activation, we want the prior to encourage some nodes to be active based on expectations propagated down from higher layers. These higher layers could be focused on grouping inputs into similar categories. We will incorporate a new loss function to encourage unsupervised clustering. This loss function minimizes the output entropy per image, but maximizes it per batch. The intuition is that minimizing entropy per image will force the network to place the image into a category, since the number of output nodes is small (e.g. ~10 for MNIST). Maximizing the entropy across batches is intended to prevent the network from placing all images into a single category, assuming there is an approximately even distribution of classes in a given batch. We will add a second layer on top of the LCA network and enforce a categorical cost. The cost is cross-entropy when there is a label or the combined entropy terms described earlier when there is not. Taking the derivative of this new cost with respect to a neuron will give us a new update rule for inference.

Our proposed model is capable of weakly-supervised learning, where only a small percentage of data examples have corresponding labels. We used the model for categorizing novel inputs using a heavily reduced set of labeled examples. Typical solutions to this problem utilize a combination of supervised and unsupervised learning objectives. The supervised objective aims to build an association between a given input and label, such that similar inputs receive the same label. The unsupervised learning objective aims to preserve a faithful representation of the input, such that the input data can be reconstructed directly from the network activations. In a typical scheme, the supervised objective is used when labels are available and the unsupervised objective is used when they are absent. In addition to these two classic objectives, we have added an additional unsupervised objective that encourages the network to confidently group the inputs into categories. In the unsupervised case we do not have a training label to verify the network’s categorization, so we instead encourage the network to be as confident as possible about the category it has assigned to the input, regardless of the accuracy of the categorization. This additional objective improves the network’s ability to categorize inputs, which is typically absent from unsupervised learning.


\subsubsection{LCA with feedback model description}
A traditional deep network layer produces an output by filtering input data through a linear weight matrix and a nonlinear thresholding function. The thresholded output is then passed to the next layer in the hierarchy. Dimension-reducing nonlinearities, such as max-pooling are also often included between layers to increase network invariance to label-preserving variations in the data as well as to prevent combinatorially increasing layer size with depth. This process continues until, ultimately, a probability distribution over possible categories is produced as the final layer’s output. For static data classification, such as image labeling, most deployed state-of-the-art networks are feedforward in that information strictly flows in one direction through the network. Additionally, the layers themselves do not demonstrate a dynamical response to the input. In our alternative approach, each layer performs a dynamical non-linear computation on the input. The first layer is an LCA layer that incorporates lateral connectivity between neurons to enforce competition, creating a descriptive, distributed sparse code of the input data. This code is produced in a recurrent fashion, where the network dynamics evolve through time to a converged representation of the input. Additionally, each LCA neuron receives input from the layer above that alters the dynamics in a context-dependent way. The resulting network representation is hierarchical and faithful to the input, such that the data can be directly reconstructed from the neuron activation values. Within-layer competition and top-down feedback encourage the network to produce a maximally descriptive code that is context-aware. The semi-supervised nature of the model will allow us to leverage raw data without the need for expensive human labeling.

For the LCA with feedback (LCAF) model, we will add an additional classification layer on top of the LCA model, as illustrated in figure \ref{fig:ch3_mlp_lcaf_architectures}.  The network minimizes one of two different energy functions, depending on whether there input image has a provided label. In the case that there is a label, the energy function is the same as was used in equation \ref{eq:ch2_sparse_energy}, with the addition a the cross-entropy loss:

\begin{equation}\label{eq:ch3_lcaf_supervised_energy}
         E =
        \overbrace{ \tfrac{1}{2} \| s - \hat{s} \|_{2}^{2} }^\text{Preserve Information} +
        \overbrace{ \lambda \sum\limits_{i=1}^{M}C(a_{i}) }^\text{Limit Activations} -
        \overbrace{ \alpha \sum\limits_{j=1}^{K} y_{j}log(\hat{y_{j}})}^\text{Cross-Entropy Cost},
\end{equation}

\noindent where $K$ is the number of categories, $\alpha$ is a tunable trade-off parameter, $y_{j}$ is a ground-truth one-hot label, and $\hat{y_{j}} = \frac{e^{-Wa_{j}}}{\sum_{n}e^{-Wa_{n}}}$ is the softmax output of the classification layer. When a label is not present, we swap out the cross entropy cost with an entropy cost. We want our model to have high confidence (low entropy) per image and high entropy across batch (because we are assuming the categories are evenly distributed). We do this by defining two entropy terms. The first computes the entropy per image by summing across the neuron indices:

\begin{align}\label{eq:ch3_lcaf_q_dist}
\begin{split}
  Q_{i,l} &= \frac{e^{-\gamma \hat{y}_{i,l}}}{\sum\limits_{k=1}^{K}e^{-\gamma \hat{y}_{k,l}}} \\
  H^{\text{neuron}}_{l} &= -\sum_{i}Q_{i,l}\log Q_{i,l}.
 \end{split}
\end{align}

The second term computes the batch entropy by summing across the batch dimension:

\begin{align}\label{eq:ch3_lcaf_p_dist}
\begin{split}
  P_{i,l} &= \frac{e^{-\gamma \hat{y}_{i,l}}}{\sum\limits_{b=1}^{B} e^{-\gamma \hat{y}_{i,b}}} \\
  H^{\text{batch}}_{i} &= -\sum_{l}P_{i,l}\log P_{i,l},
 \end{split}
\end{align}

\noindent where $B$ is the batch size. Now we can combine these terms for our unsupervised entropy loss:

\begin{equation}\label{eq:ch3_lcaf_unsupervised_energy}
         E =
        \overbrace{ \tfrac{1}{2} \| s - \hat{s} \|_{2}^{2} }^\text{Preserve Information} +
        \overbrace{ \lambda \sum\limits_{i=1}^{M}C(a_{i}) }^\text{Limit Activations} +
        \overbrace{ \alpha_{1} \sum\limits_{l=1}^{K} H^{\text{neuron}}_{l} - \alpha_{2} \sum\limits_{i=1}^{B}H^{\text{batch}}_{i}}^\text{Entropy Cost},
\end{equation}

\noindent where $\alpha_{1}$ and $\alpha_{2}$ are tunable loss trade-off parameters. Our LCA inference equation follows the same derivation from equation \ref{eq:ch2_lca_deda_simple}, with an added term computed from the derivative of the entropy or cross-entropy costs with respect to the activity vector, $a$.

%TODO: add lca update rule (https://docs.google.com/presentation/d/1Dy_Dy1uSnLC3FEWXczgdKGxSejRUYQmfHnwPtY5LA8Y/edit#slide=id.g12e96bb738_0_211 for cross-entropy)

\begin{figure}\label{fig:ch3_mlp_lcaf_architectures}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/mlp_lcaf_architectures.png}
    \caption{\textbf{Training a classifier on LCA outputs.} We propose a two-layer LCA architecture that learns a set of weights using a semi-supervised objective in the second layer. We compare this against a standard 2-layer MLP architecture.}
\end{figure}

\subsection{Experiments on MNIST dataset}
Our fist experiment is to verify that the classifier is able to train using sparse codes as input. The following table gives the MNIST test accuracy using a fully supervised label set. The LCA model was pre-trained on MNIST without labels and then a single layer classifier was trained on the activity vector, $a$.

\begin{table}[]
\begin{tabular}{lllll}
 & \textbf{MLP} & \textbf{\begin{tabular}[c]{@{}l@{}}MLP with\\ random $\Phi$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}MLP with\\ LCA $\Phi$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}LCA with\\ classifier\end{tabular}} \\ \cline{2-5} 
\multicolumn{1}{l|}{\textbf{Test Error}} & \multicolumn{1}{l|}{411} & \multicolumn{1}{l|}{2979} & \multicolumn{1}{l|}{1723} & \multicolumn{1}{l|}{331} \\ \cline{2-5} 
\multicolumn{1}{l|}{\textbf{Test Accuracy}} & \multicolumn{1}{l|}{95.89\%} & \multicolumn{1}{l|}{70.21\%} & \multicolumn{1}{l|}{82.77\%} & \multicolumn{1}{l|}{96.69\%} \\ \cline{2-5} 
\end{tabular}
\caption{\textbf{LCA helps with MNIST classification.} This table shows that a single layer classifier trained on LCA encodings of the MNIST digit dataset outperforms a two layer classifier trained directly on MNIST pixels. The first column is the results for a two-layer classically trained MLP. The second is the same, except that the first layer weights were frozen with random initialization. The third had the first layer weights frozen to those that were trained with LCA, but did not utilize sparse inference. The final column is the results for a single-layer classifier trained on the LCA activations.}
\label{tab:ch3_mnist_accuracy}
\end{table}

Next we modified the MNIST dataset such that a varying percentage of the labels are removed to test our network’s ability to generalize and label unseen digit images. In table \ref{tab:ch3_restricted_mnist_accuracy}, we show that adding the supervised cross-entropy feedback to sparse inference had little effect on the limited label regime. However, combining the supervised cross-entropy feedback with the unsupervised entropy feedback resulted in an improved score with very few labeled examples. We note that these scores were not cross-validated, and therefore we cannot assign confidence scores to the accuracy reported. The error rates found from previous studies were 0.1\%, 1\%, and 5\% for 50000, 100, and 20 labeled examples respectively \parencite{rasmus2015semi}.

\begin{table}[]
\begin{tabular}{llll}
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Number of labeled\\ training examples\end{tabular}}} & \multicolumn{1}{c}{\textbf{LCA}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}LCAF\\ sup only\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}LCAF\\ sup and unsup\end{tabular}}} \\ \cline{2-4} 
\multicolumn{1}{l|}{\textbf{50,000}} & \multicolumn{1}{l|}{96\%} & \multicolumn{1}{l|}{96\%} & \multicolumn{1}{l|}{96\%} \\ \cline{2-4} 
\multicolumn{1}{l|}{\textbf{100}} & \multicolumn{1}{l|}{68\%} & \multicolumn{1}{l|}{67\%} & \multicolumn{1}{l|}{65\%} \\ \cline{2-4} 
\multicolumn{1}{l|}{\textbf{20}} & \multicolumn{1}{l|}{33\%} & \multicolumn{1}{l|}{33\%} & \multicolumn{1}{l|}{38\%} \\ \cline{2-4} 
\end{tabular}
\caption{\textbf{Feedback helps with MNIST classification.} We compare the LCA with classifier model against two variants: One with strictly supervised feedback (middle column) and another with supervised and unsupervised feedback (right column). Although the feedback does not appear to help when there are a large number of labeled examples, it does show a positive effect when the number of labeled examples is restricted.}
\label{tab:ch3_restricted_mnist_accuracy}
\end{table}

To test whether the unsupervised feedback term gave a meaningful signal, we measured the distances among sparse codes produced without feedback, with supervised feedback, and with unsupervised feedback. Figure \ref{fig:ch3_feedback_code_distances} shows that the Hamming distance between codes produced with either form of feedback are smaller than the distance from a code produced with feedback to one produced without feedback. This tells us that the feedback itself changes the code produced, and also that the unsupervised feedback produces a code that is similar to the supervised feedback code, which suggests that the unsupervised feedback is a good proxy for the supervised signal.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/feedback_code_distances.png}
    \caption{\textbf{Unsupervised feedback during LCA inference produces similar codes to supervised feedback.} We measure the number of neurons that crossed their activation threshold (from active to inactive or vice versa) in terms of a Hamming distance. The distances between the codes produced without feedback and with either form of feedback (left two bars) are larger than the distance between the codes produced with supervised and unsupervised feedback (right bar). This tells us that the unsupervised entropy feedback produces a meaningful signal that is similar to that produced by supervised cross-entropy feedback. The bars height indicates mean hamming distance for 100 images and error bars indicate standard error of the mean.}
    \label{fig:ch3_feedback_code_distances}
\end{figure}

We also tested the weights learned with and without feedback during inference. In this experiment, the dictionary update rule is the same as was described in equation \ref{eq:ch2_phi_update}, but the sparse inference process was modified. Figure \ref{fig:ch3_feedback_nofeedback_features} shows that the feedback process influences the first layer weights learned to produce more prototypical digits. Additionally, the first layer neurons that are most strongly connected to a specific second layer classification neuron have a higher degree of correspondence to the classification category when feedback was used during inference.

\begin{figure}\label{fig:ch3_feedback_nofeedback_features}
    \centering
    \begin{subfigure}[b]{0.5\textwidth}\label{fig:ch3_nofeedback_features}
      \includegraphics[width=\textwidth]{figures/lca_nofeedback_classifier_features.png}
      \caption{Without feedback}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}\label{fig:ch3_feedback_features}
      \includegraphics[width=\textwidth]{figures/lca_feedback_classifier_features.png}
      \caption{With supervised feedback}
    \end{subfigure}
    \caption{\textbf{The weights learned by the LCA classifier are affected by the supervised feedback signal.} The subfigures show the basis functions, $\Phi$, for the top and bottom strongest connected first layer neurons to each classification output neuron. (a) Inference was performed with supervised feedback. (b) Inference was performed without feedback. Each 4x4 grid corresponds to a particular classification label. Images above the red line are the basis functions for the 8 neurons that are most strongly connected to the given classification neuron and below the red line are the bottom 8. The basis functions themselves change with feedback, and the structure of the top connected basis functions is better matched to the digit label when feedback is used.}
\end{figure}

\subsection{Conclusion}
There is a strong need for statistical models to learn from data without human curated labels because of the considerable expense of generating such labels and to avoid the unintended biases that human labeling introduces. However, most deep neural networks - the gold standard in modern machine learning - are trained in a supervised manner and are predicated on a narrowly specified task, such as labeling objects in images or videos. We propose a semi-supervised deep network to learn a hierarchical representation of visual data with or without corresponding labels. Instead of a narrowly specified task, the primary objective of our model is constructing a general, hierarchical and efficient code of the data that can be applied to a myriad of different tasks. This section showcased a promising research direction of using supervised and unsupervised entropy signals to direct LCA inference. Much more work needs to be done to investigate how feedback is influencing inference and learning as well as making comparisons to alternative methods.


\section{Subspace LCA}
Talk about this work.