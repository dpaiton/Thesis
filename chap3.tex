\chapter{Hierarchical LCA}

\section{Related hierarchical sparse coding models}\label{sec:ch3_related_models}
A hierarchical model of natural scenes should produce a general representation of input data that spans all layers. The bottom of the hierarchy should contain information about the details of the scene, and as one ascends the hierarchy, one should see a more general, abstract description. Information should move up the hierarchy in the form of inference and down the hierarchy as expectations or priors. Resolving ambiguities at the top should be easier as there are more regularities and there is more context. These should help inform inference in lower layers to resolve more difficult ambiguities about details of the scene. For this to occur, the network should perform inference at all layers simultaneously.

Broadly speaking, there have been many hierarchical unsupervised learning architectures in the literature. Here we will highlight work that is particularly relevant considering the goals of the previous paragraph. Lee and Mumford \citeyearpar{lee2003hierarchical} propose a model that comes very close to what was described in the previous paragraph. They simplify the model by forcing each layer to only receive input from the layer below & above, as in a Markov chain. Expectations are propagated down to alter priors of lower layers in a dynamic, context-sensitive manner. Feedback connections influence the inference process, causing a layer to converge on a solution that fits the expectations above. They use dynamic sampling algorithms to represent Bayesian inference, but we believe it is possible to construct a version that is true to the theory and uses sparse coding as a core computational framework. We identify this as an exciting area of future research. Karklin and Lewicki \citeyearpar{karklin2003learning} propose a hierarchical probabilistic model that is an extension of the ICA model. They start with learning a complete orthogonal basis with ICA, and then learn ``variance bases'' that model higher-order structure in images. Their model produces a hierarchical sparse distributed code of natural scenes, although they do not perform inference but instead directly compute first layer activations in a feed-forward architecture. They are able to learn to represent higher-order structure such as elongated edges with a nonlinear inference process because the joint coefficient distribution is replaced with a hierarchical prior. An exciting extension of this work would be to extend the LCA model to include their probabilistic second layer. The variances learned in the second layer can then be used to guide LCA inference. Shan and Cottrell \citeyearpar{shan2013efficient}  demonstrate a hierarchical model by alternating PCA-like and ICA transforms. They learn representations that resemble those of other hierarchical models by using PCA to reduce the dimensionality of ICA outputs and then learn an overcomplete ICA \parencite{le2011ica} representation of the PCA activations. Although their model is not derived from a probabilistic framework and computes linear activations, it is a promising proof of concept for a framework that alternates dimensionality reduction and expansion to learn higher-order structure from natural images. Cadieu and Olshausen \citeyearpar{cadieu2008learning} propose a two-layer sparse coding network that explicitly disentangles form and motion in the second layer when trained on video input. The first layer is a sparse coding model that is trained with complex valued dictionary elements. They then factorize the first layer output into two sets of second layer inputs that encode the time-derivative of the amplitude and phase components to explicitly separate form and motion, respectively. The second layer units also perform sparse coding on the logarithm of factorized quantities from the first layer. They demonstrate that form selective neurons develop invariance properties from the time-varying signal. We believe the use of time information from natural videos is a key component of learning a hierarchical representation of natural scenes that has high correspondence to what we see in biology. A convolutional LCA variant called the Deconvolutional Competitive Algorithm (DCA) is proposed in \parencite{paiton2015deconvolutional}. This model deconstructs images into a three-layer hierarchical code. The network performs inference in all three layers simultaneously, and all three layers compete to reconstruct the input. By configuring the strides and patch sizes appropriately, the network was designed such that each layer contributes a different degree of spatial detail. The lowest layer reconstructed high spatial frequency, grayscale information while the highest layer reconstructed low spatial frequency and color information. However, the generative process is linear and therefore could be represented with a single dictionary. Ideally, we would like the generative process to be able to account for the non-linear structure in images. The sparse manifold transform, proposed by \parencite{chen2018sparse}, is an extendable hierarchical sparse coding network that models the underlying manifold structure of time-varying natural scenes (i.e. videos). The model combines dimensionality expansion using sparse coding and reduction using manifold embedding. The transform is also invertible via a non-linear generative process, such that linear translations in the embedded space result in non-linear transformations in the image space. The learned dimensionality reduction step can be thought of as an alternative to pooling operations commonly found in deep learning architectures.

%\subsection{PCA dimensionality reduction to learn second layer units}
%Hyvarinen & Hoyer strong PCA work; my project implementing this with LCA


\section{Weakly-Supervised Feature Learning}\label{sec:ch3_weak_supervised_learning}

\subsection{Introduction}
We evolved a visual sense to allow us to understand the external causes of incident light. The sensor modality is not designed to construct a veridical representation of the world \parencite{gollisch2010eye}. Instead, it is designed to allow us to identify objects of significance and act upon them. Our visual system is intimately connected with the statistics of light as it propagates through our natural world. These statistics have been analyzed extensively by scientists exploring images and videos of natural scenes (see chapter \ref{ch:intro}). The sparse coding model represents an attempt to build on the knowledge gained from studying the statistics of natural scenes to better understand our visual system. In the field of visual unsupervised machine learning, a cornucopia of models have been proposed to learn the statistics of natural scenes without human labels, or ``supervision'' \parencite{baldi2012autoencoders, bengio2012unsupervised, goodfellow2016deep}. It is important to recognize that the unsupervised objective function of most of these models does in fact ask for a veridical representation of the inputs, without any real consideration for other ecological significance for the latent code produced. None the less, when combine with a family of constraints that include minimum entropy, maximum compression, and minimal energy expenditure, autoencoder models can exhibit interesting properties that are also found in biological vision systems. In this chapter, we consider the LCA as an autoencoder. Like an autoencoder, the LCA receives inputs, transforms them into a latent code, and produces reconstructions. We are interested in understanding how useful LCA can be for the machine learning field. One way to assess this is to look at semi-supervised learning. Here the objective is the same as for supervised learning, where we want to associate images with some predetermined category label. However, the catch is that many of the training images do not have ground truth labels assigned to them. A fully supervised model would not be able to use these, and would suffer from limited training examples. Here we show that LCA can be used as an agent to improve semi-supervised learning results. We also demonstrate how an alternate objective, like labeling objects in the world, can be used in the LCA dynamics to modify inference and dictionary learning.

\subsection{Weakly-supervised learning}
%[https://docs.google.com/presentation/d/1Dy_Dy1uSnLC3FEWXczgdKGxSejRUYQmfHnwPtY5LA8Y/edit#slide=id.g12e96bb738_0_271]
%[https://docs.google.com/presentation/d/1CcFmB1AUIEWU_rKtIaiRM79QGMetjYhnjKfv58hDD8Y/edit#slide=id.g19049d0ee0_0_22]
%[https://docs.google.com/document/d/13IzufcIS9M9HTCKsQGPSWmCB7lbm4axBAVbGeEml-ks/edit?usp=sharing]
%[https://docs.google.com/presentation/d/1bW__4dYIlrrbiV55Y1WdKcI_jxYqCSPXVpQjnumrDfo/edit#slide=id.g13eab2f708_0_83 slide 11]

Human generated labels are extremely expensive to produce and often biased. A learning paradigm that avoids this process is unsupervised learning, but it is not directly applicable to the machine learning task of assigning labels to, or categorizing, data. Weakly-supervised learning is a sub-field that aims to combine the benefits of unsupervised and supervised learning. An ideal model should learn to categorize (e.g. cluster) data without ground-truth labels while still maintaining a faithful representation. As we demonstrate later in chapter \ref{ch:iso}, sparse coding produces a code that is both descriptive and faithful to the image content. Here, we wish to modify the sparse coding model to utilize limited label information about an input scene.

\subsubsection{Features learned in unsupervised frameworks match those learned in supervised frameworks}
The Discriminative Recurrent Sparse Auto-Encoder (DrSAE, \cite{rolfe2013discriminative}) behaves similarly to sparse coding and has been shown to be successful at semi-supervised learning. Unlike LCA, the encoder, decoder, and lateral connectivity weights of DrSAE are unconstrained and trained independently. However, when trained on the MNIST dataset \parencite{lecun1998mnist}, the DrSAE learns weights that closely match what we impose for sparse coding. That is, the decoder weights have high correspondence to the transpose of the encoder weights and the lateral connectivity weights have high correspondence with the Gramian of the encoder weights. Additionally, when semi-supervised training is performed, the model learns an emergent hierarchical architecture. Figure \ref{fig:ch3_lenet_lca_drsae_weights} shows that the features learned by the unsupervised LCA and DrSAE models have a high degree of structural similarity to those learned by a supervised model \parencite{lecun1998gradient}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lenet_lca_drsae_weights.png}
    \caption{\textbf{Unsupervised learning leads to similar features as supervised learning.} The features learned from unsupervised training on MNIST using LCA \parencite{rozell2008sparse} and DrSAE \parencite{rolfe2013discriminative} have similar structure to those learned using the supervised LeNet model \parencite{lecun1998gradient}. It requires a significant amount of regularization in the form of dropout and weight decay to learn structured weights with most supervised architectures, although the unsupervised models compared here do not require dropout or weight decay.}
    \label{fig:ch3_lenet_lca_drsae_weights}
\end{figure}

The DrSAE model was also extended to a semi-supervised learning framework. Given their success, we constructed a similar framework with the LCA. The LCA dynamics are derived from a principled energy function, so we were able to extend the framework by adding semi-supervised loss terms to the energy function. Typically in sparse coding the sparsity enforcing term is applied uniformly, penalizing all nodes. Instead of the prior limiting the total activation, we want the prior to encourage some nodes to be active based on expectations propagated down from higher layers. These higher layers could be focused on grouping inputs into similar categories.

Our proposed model is capable of weakly-supervised learning, where only a small percentage of data examples have corresponding labels. We trained the models to categorize novel inputs using a heavily reduced set of labeled examples. Typical solutions to this problem utilize a combination of supervised and unsupervised learning objectives. The supervised objective aims to build an association between a given input and label, such that similar inputs receive the same label. The unsupervised learning objective aims to preserve a faithful representation of the input, such that the input data can be reconstructed directly from the network activations. In a typical scheme, the supervised objective is used when labels are available and the unsupervised objective is used when they are absent. In addition to these two classic objectives, we have added an additional unsupervised objective that encourages the network to confidently group the inputs into categories. This additional objective improves the network’s ability to categorize inputs, which is typically absent from unsupervised learning. The loss function minimizes the output entropy per image, but maximizes it per batch. The intuition is that minimizing entropy per image will force the network to confidently place the image into a category, since the number of output nodes is small (e.g. $\sim$10 for MNIST). Maximizing the entropy across batches is intended to prevent the network from placing all images into a single category, assuming there is an approximately even distribution of classes in a given batch. We implemented this loss by adding a second layer on top of the LCA network that produced proposed categorical outputs. The network is trained using cross-entropy when there are labels or the combined entropy terms described earlier when there are not. Taking the derivative of this new cost with respect to a neuron will give us a new update rule for sparse inference.


\subsubsection{LCA with feedback}
A traditional deep network layer produces an output by filtering input data through a linear weight matrix and a nonlinear thresholding (i.e. activation) function. The thresholded output is then passed to the next layer in the hierarchy. Dimension-reducing nonlinearities, such as max-pooling are also often included between layers to increase network invariance to label-preserving variations in the data as well as to prevent combinatorially increasing layer size with depth. This process continues until, ultimately, a probability distribution over possible categories is produced as the final layer’s output. For static data classification, such as image labeling, most deployed state-of-the-art networks are feedforward in that information strictly flows in one direction through the network. Consequently, the layers themselves do not produce a dynamical response to the input. In our alternative approach, the first of our network layer performs a dynamical non-linear computation on the input, which modifies the second layer outputs at each iteration. The first layer is an LCA layer that incorporates lateral connectivity between neurons to enforce competition, creating a descriptive, distributed sparse code of the input data (see chapter \ref{ch:lca} for details). This code is produced in a recurrent fashion, where the network dynamics evolve through time to a converged representation of the input. Additionally, each LCA neuron receives input from the layer above that alters the dynamics in a context-dependent way. The resulting network representation is hierarchical and faithful to the input, such that the data can be directly reconstructed from the first layer neuron activation values. Within-layer competition and top-down feedback encourage the LCA to produce a maximally descriptive code that is context-aware. The semi-supervised nature of the model will allow us to leverage raw data without the need for expensive human labeling.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/mlp_lcaf_architectures.png}
    \caption{\textbf{Training a classifier on LCA outputs.} We propose a two-layer LCA architecture that learns a set of weights using a semi-supervised objective in the second layer. We compare this against a standard 2-layer MLP architecture.}
    \label{fig:ch3_mlp_lcaf_architectures}
\end{figure}

For the LCA with feedback (LCAF) model, we will add an additional classification layer on top of the LCA model, as illustrated in figure \ref{fig:ch3_mlp_lcaf_architectures}. The network minimizes one of two different energy functions, depending on whether the input image has an associated label. In the case that there is a label, the energy function is the same as was used in equation \eqref{eq:ch2_sparse_energy}, with the addition of a cross-entropy term:

\begin{equation}\label{eq:ch3_lcaf_supervised_energy}
         E =
        \overbrace{ \tfrac{1}{2} \| s - \hat{s} \|_{2}^{2} }^\text{Preserve Information} +
        \overbrace{ \lambda \sum\limits_{i=1}^{M}C(a_{i}) }^\text{Limit Activations} -
        \overbrace{ \alpha \sum\limits_{j=1}^{K} y_{j}log(\hat{y_{j}})}^\text{Cross-Entropy Cost},
\end{equation}

\noindent where $K$ is the number of categories, $\alpha$ is a tunable trade-off parameter, $y_{j}$ is a ground-truth one-hot label, and $\hat{y_{j}} = \frac{e^{-Wa_{j}}}{\sum_{n}e^{-Wa_{n}}}$ is the softmax output of the classification layer. When a label is not present, we swap out the cross-entropy cost with an entropy cost. We want our model to have high confidence (low entropy) per image and high entropy across batch (because we are assuming the categories are evenly distributed). We do this by defining two entropy terms. The first computes the entropy per image by summing across the neuron indices:

\begin{align}\label{eq:ch3_lcaf_q_dist}
\begin{split}
  Q_{i,l} &= \frac{e^{-\gamma \hat{y}_{i,l}}}{\sum\limits_{k=1}^{K}e^{-\gamma \hat{y}_{k,l}}} \\
  H^{\text{neuron}}_{l} &= -\sum_{i}Q_{i,l}\log Q_{i,l}.
 \end{split}
\end{align}

The second term computes the batch entropy by summing across the batch dimension:

\begin{align}\label{eq:ch3_lcaf_p_dist}
\begin{split}
  P_{i,l} &= \frac{e^{-\gamma \hat{y}_{i,l}}}{\sum\limits_{b=1}^{B} e^{-\gamma \hat{y}_{i,b}}} \\
  H^{\text{batch}}_{i} &= -\sum_{l}P_{i,l}\log P_{i,l},
 \end{split}
\end{align}

\noindent where $B$ is the batch size. Now we can combine these terms for our unsupervised entropy loss:

\begin{equation}\label{eq:ch3_lcaf_unsupervised_energy}
         E =
        \overbrace{ \tfrac{1}{2} \| s - \hat{s} \|_{2}^{2} }^\text{Preserve Information} +
        \overbrace{ \lambda \sum\limits_{i=1}^{M}C(a_{i}) }^\text{Limit Activations} +
        \overbrace{ \alpha_{1} \sum\limits_{l=1}^{K} H^{\text{neuron}}_{l} - \alpha_{2} \sum\limits_{i=1}^{B}H^{\text{batch}}_{i}}^\text{Entropy Cost},
\end{equation}

\noindent where $\alpha_{1}$ and $\alpha_{2}$ are tunable loss trade-off parameters. Our LCA inference equation follows the same derivation from equation \eqref{eq:ch2_lca_deda_simple}, with an added term computed from the derivative of the entropy or cross-entropy costs with respect to the activity vector, $a$.

%TODO: add lca update rule (https://docs.google.com/presentation/d/1Dy_Dy1uSnLC3FEWXczgdKGxSejRUYQmfHnwPtY5LA8Y/edit#slide=id.g12e96bb738_0_211 for cross-entropy)

\subsection{Experiments on MNIST dataset}
Our fist experiment is to verify that the classifier is able to train using sparse codes as input. The following table gives the MNIST test accuracy using a fully supervised label set. The LCA model was pre-trained on MNIST without labels and then a single layer classifier was trained on the activity vector, $a$.

\begin{table}[]
\begin{tabular}{lllll}
 & \textbf{MLP} & \textbf{\begin{tabular}[c]{@{}l@{}}MLP with\\ random $\Phi$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}MLP with\\ LCA $\Phi$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}LCA with\\ classifier\end{tabular}} \\ \cline{2-5} 
\multicolumn{1}{l|}{\textbf{Test Error}} & \multicolumn{1}{l|}{411} & \multicolumn{1}{l|}{2979} & \multicolumn{1}{l|}{1723} & \multicolumn{1}{l|}{331} \\ \cline{2-5} 
\multicolumn{1}{l|}{\textbf{Test Accuracy}} & \multicolumn{1}{l|}{95.89\%} & \multicolumn{1}{l|}{70.21\%} & \multicolumn{1}{l|}{82.77\%} & \multicolumn{1}{l|}{96.69\%} \\ \cline{2-5} 
\end{tabular}
\caption{\textbf{LCA helps with MNIST classification.} This table shows that a single layer classifier trained on LCA encodings of the MNIST digit dataset outperforms a two layer classifier trained directly on MNIST pixels. The first column is the results for a two-layer classically trained MLP. The second is the same, except that the first layer weights were frozen with random initialization. The third had the first layer weights frozen to those that were trained with LCA, but did not utilize sparse inference. The final column is the results for a single-layer classifier trained on the LCA activations.}
\label{tab:ch3_mnist_accuracy}
\end{table}

Next we modified the MNIST dataset such that a varying percentage of the labels are removed to test our network’s ability to generalize and label unseen digit images. In table \ref{tab:ch3_restricted_mnist_accuracy}, we show that adding the supervised cross-entropy feedback to sparse inference had little effect on the limited label regime. However, combining the supervised cross-entropy feedback with the unsupervised entropy feedback resulted in an improved score with very few labeled examples. We note that these scores were not cross-validated, and therefore we cannot assign confidence scores to the accuracy reported. The error rates found from previous studies were 0.1\%, 1\%, and 5\% for 50000, 100, and 20 labeled examples respectively \parencite{rasmus2015semi}. We expect that our error rates would be close to those previously reported.

\begin{table}[]
\begin{tabular}{llll}
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Number of labeled\\ training examples\end{tabular}}} & \multicolumn{1}{c}{\textbf{LCA}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}LCAF\\ sup only\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}LCAF\\ sup and unsup\end{tabular}}} \\ \cline{2-4} 
\multicolumn{1}{l|}{\textbf{50,000}} & \multicolumn{1}{l|}{96\%} & \multicolumn{1}{l|}{96\%} & \multicolumn{1}{l|}{96\%} \\ \cline{2-4} 
\multicolumn{1}{l|}{\textbf{100}} & \multicolumn{1}{l|}{68\%} & \multicolumn{1}{l|}{67\%} & \multicolumn{1}{l|}{65\%} \\ \cline{2-4} 
\multicolumn{1}{l|}{\textbf{20}} & \multicolumn{1}{l|}{33\%} & \multicolumn{1}{l|}{33\%} & \multicolumn{1}{l|}{38\%} \\ \cline{2-4} 
\end{tabular}
\caption{\textbf{Feedback helps with MNIST classification.} We compare the LCA with classifier model against two variants: One with strictly supervised feedback (middle column) and another with supervised and unsupervised feedback (right column). Although the feedback does not appear to help when there are a large number of labeled examples, it does show a positive effect when the number of labeled examples is restricted.}
\label{tab:ch3_restricted_mnist_accuracy}
\end{table}

To test whether the unsupervised feedback term produces a meaningful signal for inference, we measured the distances among sparse codes produced without feedback, with supervised feedback, and with unsupervised feedback. Figure \ref{fig:ch3_feedback_code_distances} shows that the Hamming distance between codes produced with either form of feedback are smaller than the distance from a code produced with feedback to one produced without feedback. Here we measure Hamming distance to be the distance between binarized vectors where a 1 indicates a neuron's membrane potential, $u$, crossed threshold (i.e. went from active to inactive or vice versa) and a 0 indicates otherwise. This tells us that the feedback itself changes the code produced, and also that the unsupervised feedback produces a code that is similar to the supervised feedback code, which suggests that the unsupervised feedback is a good proxy for the supervised signal.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/feedback_code_distances.png}
    \caption{\textbf{Unsupervised feedback during LCA inference produces similar codes to supervised feedback.} We measure the number of neurons that crossed their activation threshold (from active to inactive or vice versa) in terms of a Hamming distance. The distances between the codes produced without feedback and with either form of feedback (left two bars) are larger than the distance between the codes produced with supervised and unsupervised feedback (right bar). This tells us that the unsupervised entropy feedback produces a meaningful signal that is similar to that produced by supervised cross-entropy feedback. The bars height indicates mean hamming distance for 100 images and error bars indicate standard error of the mean.}
    \label{fig:ch3_feedback_code_distances}
\end{figure}

We also tested the weights learned with and without feedback during inference. In this experiment, the dictionary update rule is the same as was described in equation \eqref{eq:ch2_phi_update}, but the sparse inference process was modified. Figure \ref{fig:ch3_feedback_nofeedback_features} shows that the feedback process influences the first layer weights learned to produce more prototypical digits. Additionally, the first layer neurons that are most strongly connected to a specific second layer classification neuron have a higher degree of correspondence to the classification category when feedback was used during inference.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/lca_nofeedback_classifier_features.png}
        \caption{Without feedback}
        \label{fig:ch3_nofeedback_features}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/lca_feedback_classifier_features.png}
        \caption{With supervised feedback}
        \label{fig:ch3_feedback_features}
    \end{subfigure}
    \caption{\textbf{The weights learned by the LCA classifier are affected by the supervised feedback signal.} The subfigures show the basis functions, $\Phi$, for the top and bottom strongest connected first layer neurons to each classification output neuron. (a) Inference was performed with supervised feedback. (b) Inference was performed without feedback. Each 4x4 grid corresponds to a particular classification label. Images above the red line are the basis functions for the 8 neurons that are most strongly connected to the given classification neuron and below the red line are the bottom 8. The basis functions themselves change with feedback, and the structure of the top connected basis functions is better matched to the digit label when feedback is used.}
    \label{fig:ch3_feedback_nofeedback_features}
\end{figure}

\subsection{Conclusion}
There is a strong need for statistical models to learn from data without human curated labels because of the considerable expense of generating such labels and to avoid the unintended biases that human labeling introduces. However, most deep neural networks - the gold standard in modern machine learning - are trained in a supervised manner and are predicated on a narrowly specified task, such as labeling specific objects in images or videos. We propose a semi-supervised network to learn a hierarchical representation of visual data with or without corresponding labels. Instead of a narrowly specified task, the primary objective of our model is constructing a general, hierarchical and efficient code of the data that can be applied to a myriad of different tasks. This section showcased a promising research direction of using supervised and unsupervised entropy signals to direct LCA inference. Future work includes investigating how feedback is influencing inference and learning as well as making comparisons to alternative methods.


\section{Subspace LCA}\label{sec:ch3_subspace_lca}
\subsection{Introduction}
We describe an extension to the LCA model that produces invariant representations of its inputs. The model is similar to Independent Subspace Analysis, described in \parencite{hyvarinen2000emergence}, but differs in several key ways. First, the LCA model allows for the first layer representation to be overcomplete, which produces a more efficient representation (see \parencite{lewicki2000learning} and chapter \ref{ch:iso}). Second, the first layer encoding process is non-linear, which gives the neurons a higher degree of nonlinearity and also improves efficiency (see chapter \ref{ch:iso}).

\subsection{Model description}
The subspace LCA follows a similar derivation to the LCA (section \ref{sec:ch2_lca}). Like the LCA, it learns a set of weights to efficiently describe natural signals, although in this variant we constrain the weights to be grouped, with the group size set as a hyper-parameter. We start with the same generative framework as in sparse coding:

\begin{equation} \label{eq:ch3_slcagenerative_model}
    s = \Phi a + \varepsilon.
\end{equation}

We will constrain our activations (and therefore weights) to non-overlapping groups that have an amplitude, $\sigma$:

\begin{align}\label{eq:ch3_a_decomp}
\begin{split}
  \sigma_{i} = ||a_{i}||_{2} = \sqrt{\sum_{j\in I}a^{2}_{ij}},
\end{split}
\end{align}

\noindent where $i$ indexes the group and $j \in I$ indexes the neuron within the group. The group amplitude is equivalent for many combinations of $a_{j \in I}$. Each of these equal combinations can be thought of as a direction of a vector in the activity space. We can formalize this with a unit-length direction vector, $z$, that has the same number of elements as our activity vector:

\begin{align}\label{eq:ch3_z_def}
\begin{split}
  z_{ik} = \frac{a_ik}{\sigma_{i}}.
\end{split}
\end{align}

We can now define a new energy function in these terms. We also add a regularization term that pressures the within-group weights to be orthogonal, which prevents the pathological solution of within-group neurons learning to have identical weights. This regularization term will also reduce competition between neurons within groups, which is scaled by the inner product between neighboring neurons' weight vectors.

\begin{equation}\label{eq:ch3_subspace_lca_energy}
    E = \frac{1}{2}\sum_{p}\left[s_{p} - \sum_{ij}\sigma_{i}z_{ij}\Phi_{ijp}\right]^{2} + \lambda \sum_{i}\sigma_{i} + \alpha \sum_{ij}\left|\sum_{p} \Phi_{ijp}\Phi_{ijp} - \mathbf{I}_{L} \right|,
\end{equation}

\noindent where $\alpha$ is a trade-off multiplier, $\mathbf{I}_{L}$ is the $L \times L$ identity matrix, and $L$ is the number of neurons in a group. Following the LCA derivation in section \ref{sec:ch2_lca}, we will next take the derivative of the energy with respect to a neuron's activity:

\begin{equation}\label{eq:ch3_subspace_deda}
    -\frac{\partial E}{\partial a_{ik}} = \sum_{p}s_{p}\phi_{ikp} - \sum_{lm}G_{iklm}a_{lm} - \lambda \frac{\partial ||a_{i}||^{2}}{\partial a_{ik}}.
\end{equation}

We can rewrite the last term in the above equation as our direction vector:

\begin{align}\label{eq:ch3_subspace_deda_to_z}
\begin{split}
    \frac{\partial ||a_{i}||^{2}}{\partial a_{ik}} &= \frac{1}{2}\left(\sigma_{i}^{2}\right)^{-\tfrac{1}{2}}2a_{ik}\\
    &= \frac{a_{ik}}{\sigma_{i}}\\
    &= z_{ik}.
\end{split}
\end{align}

In parody with the LCA derivation, we group the self inhibition terms:

\begin{align}\label{eq:ch3_f_of_a}
\begin{split}
    f_{\lambda}(a_{ik}) &= a_{ik} + \lambda z_{ik}\\
    &= \overbrace{\sigma_{i}+\lambda}^\text{amplitude}\overbrace{z_{ik}}^\text{direction}\\
    &= a_{ik}(1 + \frac{\lambda}{\sigma_{i}}).
\end{split}
\end{align}

We again assign our membrane potential, $u$ as $f_{\lambda}(a_{ik})$:

\begin{equation}\label{eq:ch3_u_def}
  u_{ik} = f_{\lambda}(a_{ik}) = (\sigma_{i} + \lambda)z_{ik},
\end{equation}

which also gives us an alternative definition of the neuron angle, $z$:

\begin{equation}\label{eq:ch3_z_u_def}
   z_{ik} = \frac{u_{ik}}{\sigma_{i} + \lambda} = \frac{u_{ik}}{||a_{ik}||_{2} + \lambda}.
\end{equation}

The resulting membrane update rule is nearly identical to equation \ref{eq:ch2_u_dot_full}, except that the lateral competition term includes group assignments:

\begin{equation}\label{eq:ch3_subspace_u_dot_def}
   \tau \dot{u_{ik}} - u_{ik} = \sum_{p}s_{p}\phi_{ikp} \sum_{lm \ne ik}G_{iklm}a_{lm}.
\end{equation}

Finally, we define the output amplitude in terms of the group threshold:

\begin{equation}\label{eq:ch3_subspace_threshold_func}
    a_{ik} = T_{\lambda}(\sigma_{i}) = \left\{
    \begin{aligned}
        0,\;\; & \sigma_{i}\; \leq\; \lambda \\
        (\sigma_{i}-\lambda)z_{ik},\;\; &sigma_{i}\; >\; \lambda.
    \end{aligned}
    \right.
\end{equation}

This tells us that all within-group neurons become active when the group amplitude surpasses the threshold, $\lambda$. Figure \ref{fig:ch3_subspace_lca_graph} shows a diagram of the group model.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/subspace_lca_graph.png}
    \caption{\textbf{Subspace LCA}. Neurons are grouped such that they learn subspaces of co-active units. Once a group amplitude passes the threshold, all neurons in the group become active.}
    \label{fig:ch3_subspace_lca_graph}
\end{figure}

\subsection{Features learned}
When trained on natural images, the weights learn to tile orientations, spatial frequencies, and positions just like regular LCA. They also learn to have within-group similarities, such as equal orientation and position.

%TODO: redo weight images to be on the same scale
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/subspace_lca_features.png}
    \caption{\textbf{Natural image features learned with subspace LCA}. Left) Subspace LCA learns features that have similar properties within group, but are different across groups. Right) The basis function angle histogram is very similar to that learned with LCA.}
    \label{fig:ch3_subspace_lca_features}
\end{figure}

Interestingly, we show in figure \ref{fig:ch3_subspace_lca_mnist_features} that the group structure naturally separates digit classes when the model is trained unsupervised on the MNIST dataset. We believe this shows promise to use this model in a semi-supervised framework similar to that described in section \ref{sec:ch3_weak_supervised_learning}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/subspace_lca_mnist_features.png}
    \caption{\textbf{MNIST features learned with subspace LCA}. The features learned with this model natural separate into digit categories, indicating that it might be a simple operation to assign images labels from the group amplitude representations.}
    \label{fig:ch3_subspace_lca_mnist_features}
\end{figure}

\subsection{Conclusion}
In this section we demonstrate a novel extension of the LCA that learns statistical dependencies of sparse codes by grouping co-active elements into subspaces. Future work includes drawing comparisons to the Independent Subspace Analysis (ISA - \cite{hyvarinen2000emergence}) and extending the model to a topographic version. Also, we believe this formulation can be extended to include ideas from dynamic routing \parencite{olshausen1993neurobiological}, where the $z$ variables ``steer'' the output representation.