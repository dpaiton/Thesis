\chapter{Weakly-Supervised Feature Learning}

\section{Introduction}

We evolved a visual sense to allow us to understand the external causes of incident light. The sensor modality is not designed to construct a veridical representation of the world [Gollisch and Meister 2010]. Instead, it is designed to allow us to identify objects of significance and act upon them. Our visual system is intimately connected with the statistics of light as it propagates through our natural world. These statistics have been analyzed extensively by scientists exploring images and videos of natural scenes [refs]. The sparse coding model represents an attempt to build on the knowledge gained from studying the statistics of natural scenes [field gabor paper] to better understand our visual system [olshausen 1996]. In the field of visual unsupervised machine learning, a cornucopia of models have been proposed to learn the statistics of natural scenes without human labels, or ``supervision''. It is important to recognize that the unsupervised objective function of most of these models does in fact ask for a veridical representation of the inputs, without any real consideration for other ecological significance for the latent code produced. None the less, when combine with a family of constraints that include minimum entropy, maximum compression, and minimal energy expenditure, autoencoder models can exhibit interesting properties that are also found in biological vision systems. In this chapter, we consider the LCA as an autoencoder. Like an autoencoder, the LCA receives inputs, transforms them into a latent code, and it produces reconstructions. We are interested in understanding how useful LCA can be for the machine learning field. One way to assess this is to look at semi-supervised learning. Here the objective is the same as for supervised learning, where we want to associate images with some predetermined category label. However, the catch is that many of the training images do not have ground truth labels assigned to them. A fully supervised model would not be able to use these, and would suffer from limited training examples. Here we show that LCA can be used as an agent to improve semi-supervised learning results. We also demonstrate how an alternate objective, like labeling objects in the world, can be used in the LCA dynamics to modify inference and dictionary learning.

Sparse coding is a model for unsupervised feature learning. Other models also exist.


\subsubsection{LISTA}
Learned ISTA.


\subsubsection{DrSAE}
Converges to SC solution.


\section{Weakly-supervised learning}
[https://docs.google.com/presentation/d/1Dy_Dy1uSnLC3FEWXczgdKGxSejRUYQmfHnwPtY5LA8Y/edit#slide=id.g12e96bb738_0_271]
[https://docs.google.com/presentation/d/1CcFmB1AUIEWU_rKtIaiRM79QGMetjYhnjKfv58hDD8Y/edit#slide=id.g19049d0ee0_0_22]
[https://docs.google.com/document/d/13IzufcIS9M9HTCKsQGPSWmCB7lbm4axBAVbGeEml-ks/edit?usp=sharing]

Learning unsupervised activations can be used for semi-supervised learning. Human labels are extremely expensive. An ideal model should learn to categorize (e.g. cluster) data without ground-truth labels while still maintaining a faithful representation. As we demonstrated in chapter \ref{ch:iso}, sparse coding produces a code that is both descriptive and faithful to the image content. Here, we wish to modify the sparse coding model to utilize limited label information about an input scene.

Deep neural networks have become nearly ubiquitous in the task of digital media search. Although their performance on search tasks has far surpassed the previous state-of-the-art, there is still a strong drive in industry and academia to develop the next generation of neural networks. Here we propose a novel, data-driven processing framework that incorporates additional computational elements inspired from our current understanding of computation in the brain. Our network utilizes unsupervised learning metrics to leverage the massive amount of available unlabeled data as well as supervised metrics for performing relevant tasks. We aim to demonstrate weakly supervised learning performance that surpasses the current state-of-the-art and improves Yahoo’s image search capabilities by learning from the wealth of unlabeled Flickr images.

We first trained the LCA on the MNIST dataset of handwritten digits. 

Typically in sparse coding the sparsity enforcing term is applied uniformly, penalizing all nodes. Instead of the prior limiting the total activation, we want the prior to encourage some nodes to be active based on expectations propagated down from previous layers. These previous layers will focus on grouping inputs into similar categories. We will incorporate a new loss function to encourage unsupervised clustering. This loss function minimizes the output entropy per image, but maximizes it per batch. The intuition is that minimizing entropy per image will force the network to place the image into a category, since the number of output nodes is small (e.g. ~10 for MNIST). Maximizing the entropy across batches is intended to prevent the network from placing all images into a single category. We will add a second layer on top of the LCA network and enforce a categorical cost. The cost is cross-entropy when there is a label or the combined entropy terms described earlier when there is not. Taking the derivative of this new cost with respect to a neuron will give us a new update rule for inference.

Our proposed model is capable of weakly-supervised learning, where only a small percentage of data examples have corresponding labels. We aim to use the model to demonstrate state-of-the-art performance for categorizing novel inputs using a heavily reduced set of labeled examples. Typical solutions to this problem utilize a combination of supervised and unsupervised learning objectives. The supervised objective aims to build an association between a given input and label, such that similar inputs receive the same label. The unsupervised learning objective aims to preserve a faithful representation of the input, such that the input data can be reconstructed directly from the network activations. In a typical scheme, the supervised objective is used when labels are available and the unsupervised objective is used when they are absent. In addition to these two classic objectives, we have added an additional unsupervised objective that encourages the network to confidently categorize the input. In the unsupervised case we do not have a training label to verify the network’s categorization, so we instead encourage the network to be as confident as possible about the category it has assigned to the input, regardless of the accuracy of the categorization. This additional objective improves the network’s ability to categorize inputs, which is typically absent from unsupervised learning.


\subsubsection{Recurrent, lateral, and top-down connectivity}
A traditional deep network layer produces an output by filtering input data through a linear weight matrix and a nonlinear thresholding function. The thresholded output is then passed to the next layer in the hierarchy. Dimension-reducing nonlinearities, such as max-pooling are also often included between layers to increase network invariance to label-preserving variations in the data as well as to prevent combinatorially increasing layer size with depth. This process continues until, ultimately, a probability distribution over possible categories is produced as the final layer’s output. For static data classification, such as image labeling, most deployed state-of-the-art networks are feedforward in that information strictly flows in one direction through the network. Additionally, the layers themselves do not demonstrate a dynamical response to the input. In our alternative approach, each layer performs a dynamical non-linear computation on the input. The layer incorporates lateral connectivity between neurons to enforce competition, creating a descriptive, distributed sparse code of the input data. This code is produced in a recurrent fashion, where the network dynamics evolve through time to a converged representation of the input. Additionally, each network layer receives input from the layer above that alters the dynamics in a context-dependent way. The resulting network representation is hierarchical and faithful to the input, such that the data can be directly reconstructed from the neuron activation values. Within-layer competition and top-down feedback encourage the network to produce a maximally descriptive code that is context-aware throughout the hierarchy. An equally important distinction is that our model can learn from data with or without supervised labels. This ability will allow us to leverage raw data without the need for expensive human labeling. We believe our proposed process will significantly improve overall classification performance.


\section{Experiments on MNIST dataset}
We will begin testing our model using the MNIST digit dataset. We will modify the dataset such that a varying percentage of the labels are removed and test our network’s ability to generalize and label unseen digit images. The following table from Rasmus et al. 2015[1] gives example test errors using the full MNIST dataset as training (“all” example), and two reduced versions of the dataset with 2% (1000 labeled images) and 0.2% (100 labeled images) of the images given labels.
This result serves as a comparison metric for the project herein. The process can then be repeated on harder datasets such as the CIFAR-100 dataset or the PASCAL VOC dataset of natural images. 
The proposed network draws inspiration from classic models for image representation as well as new variations focused on learning hierarchical representations[2-8]. Following the methods introduced by Yan Karklin and Michael Lewicki[5], we will construct a model with layer dynamics that evolve as a result of three input types: a driving bottom-up, feedforward connection; a modulating, context-dependent, feedback connection; and a lateral connection that enforces competition within a layer. The layer dynamics will be described as an extension of the Locally Competitive Algorithm (LCA)[9], a popular method for generating sparse codes from raw data. The network will be composed of layers following LCA-like dynamics with interconnecting weights that are learned by back-propagation of gradients. These gradients will originate from an energy function that will include unsupervised and, when available, supervised objectives:

\begin{equation}\label{weakly_sup_energy}
E = ..
\end{equation}

In this energy function, the first term corresponds to reconstruction error using the activity values (zl=0) of the lowest layer in the hierarchy, where x is the input image and Dis a decoding weight matrix, and is a sparsity tradeoff parameter. The second term is a weighted sparsity-enforcing term that rescales the sparsity constraint per element using . The rescaling term, , is set by zl+1according to the equation log() = jBijzjl+1. The third term comes from taking the log likelihood of the expectation signal, P(zi|i). Finally, a sparsity constraint is placed on the second layer in the system. Although this energy function is defined for a two-layer system, it could easily be extended to an L-layer system. Following L sparse-coding layers, a category layer can be used for task-specific learning. This layer computes an output that corresponds to the inner product between a classification weight matrix and the neural activations of layer L: CzL. The resulting energy function will contain a supervised loss function, when available, that compares the cross entropy between the classifier output (CzL) and the ground-truth label. When ground-truth labels are not available, the energy function will include an unsupervised loss function that computes the entropy of the classifier output. In either case, minimizing the task-specific objective will influence both the connection weights as well as the layer dynamics during inference.

All network models will be constructed using the Google TensorFlow library.


\section{Conclusion}
There is a strong need for statistical models to learn from data without human curated labels because of the considerable expense of generating such labels and to avoid the unintended biases that human labeling introduces. However, most deep neural networks - the gold standard in modern machine learning - are trained in a supervised manner and are predicated on a narrowly specified task, such as labeling objects in images or videos. We propose a semi-supervised deep network to learn a hierarchical representation of visual data with or without corresponding labels. Instead of a narrowly specified task, the primary objective of our model is constructing a general, hierarchical and efficient code of the data that can be applied to a myriad of different tasks.