\chapter{Conclusion}
In this thesis we present the Locally Competitive Algorithm as a model for computation in V1. While other works \parencite{zhu2013visual,olshausen1997sparse,vinje2000sparse} have demonstrated that the LCA matches well with response properties of V1 simple cells and population activity, here we dig deeper in to the actual model computation to better understand it and expand on it. Lateral connectivity is a core component of the LCA and allows for population nonlinear interactions that facilitate an explaining-away property when encoding natural stimulus. We argue that this type of connectivity, which is also ubiquitous in neural circuits, is highly beneficial for biologically relevant and machine learning relevant tasks. We show that sparse inference produces more a more efficient, robust, and selective encoding over networks without lateral connectivity and recurrence. We demonstrate that the LCA can be expanded to produce hierarchical representations, and present several promising directions for future work in this direction. Our hope is that a detailed analysis of the model will demystify its complexity and encourage computational neuroscientists to abandon the normative pointwise linear/nonlinear neuron models in favor of those with population nonlinear interactions and dynamic inference.